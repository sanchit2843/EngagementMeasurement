{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention-openface.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchit2843/Attentiondetection/blob/master/openface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWNePBQmBY6y",
        "colab_type": "code",
        "outputId": "8e217f1d-becb-4a61-843f-a64df35e8437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1AS0HdPFysd4kTne-taXWPkuDP7qR4_Hn',\n",
        "                                    dest_path='./data.zip',\n",
        "                                    unzip=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1AS0HdPFysd4kTne-taXWPkuDP7qR4_Hn into ./data.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sbl8TjJFbdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [' gaze_0_x', ' gaze_0_y', ' gaze_0_z', ' gaze_1_x', ' gaze_1_y',' gaze_1_z'\n",
        "      , ' gaze_angle_x', ' gaze_angle_y',' pose_Tx', ' pose_Ty', ' pose_Tz', ' pose_Rx'\n",
        "      , ' pose_Ry', ' pose_Rz',' p_scale', ' p_rx', ' p_ry', ' p_rz', ' p_tx', ' p_ty'\n",
        "      , ' p_0', ' p_1',' p_2', ' p_3', ' p_4', ' p_5', ' p_6', ' p_7', ' p_8', ' p_9'\n",
        "      , ' p_10',' p_11', ' p_12', ' p_13', ' p_14', ' p_15', ' p_16', ' p_17', \n",
        "      ' p_18', ' p_19', ' p_20', ' p_21', ' p_22', ' p_23', ' p_24', ' p_25', ' p_26',\n",
        "       ' p_27', ' p_28', ' p_29', ' p_30', ' p_31', ' p_32', ' p_33',\n",
        "       ' AU01_r', ' AU02_r', ' AU04_r', ' AU05_r', ' AU06_r', ' AU07_r',\n",
        "       ' AU09_r', ' AU10_r', ' AU12_r', ' AU14_r', ' AU15_r', ' AU17_r',\n",
        "       ' AU20_r', ' AU23_r', ' AU25_r', ' AU26_r', ' AU45_r', ' AU01_c',\n",
        "       ' AU02_c', ' AU04_c', ' AU05_c', ' AU06_c', ' AU07_c', ' AU09_c',\n",
        "       ' AU10_c', ' AU12_c', ' AU14_c', ' AU15_c', ' AU17_c', ' AU20_c',\n",
        "       ' AU23_c', ' AU25_c', ' AU26_c', ' AU28_c', ' AU45_c']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhakY_9XRpLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "from tqdm.autonotebook import tqdm\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "def create_dataset(path,label_file,res):\n",
        "  os.makedirs(res,exist_ok = True)\n",
        "  data_complete = {}\n",
        "  label = pd.read_csv(label_file).values\n",
        "  for i in tqdm(range(len(label))):\n",
        "    path_file = os.path.join(path,label[i][0][:6],label[i][0][:-4],label[i][0][:-3] + 'csv')\n",
        "    if(os.path.exists(path_file)):\n",
        "      pass\n",
        "    else:\n",
        "      continue\n",
        "    file = path_file.split('/')[-1]\n",
        "    try:\n",
        "      data = pd.read_csv(path_file)\n",
        "    except:\n",
        "      continue\n",
        "    if(data.shape[0] == 300):\n",
        "      data_complete[file] = (normalize(data[columns].values,axis = 1),label[i][2])\n",
        "    if(i%1000 == 0 and i != 0):\n",
        "      torch.save(data_complete,os.path.join(res,'train_{}.pt'.format(int(i/100))))\n",
        "      data_complete = {}\n",
        "  torch.save(data_complete,os.path.join(res,'train_{}.pt'.format(math.ceil(i/100))))\n",
        "  flag = 0\n",
        "  for i in os.listdir(res):\n",
        "    if(os.path.isfile(os.path.join(res,i))):\n",
        "      if(flag == 0):\n",
        "        d1 = torch.load(os.path.join(res,i))\n",
        "        flag = 1\n",
        "      else:\n",
        "        d2 = torch.load(os.path.join(res,i))\n",
        "        d1.update(d2)\n",
        "  torch.save(d1,res + '.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRC_ssjaR66t",
        "colab_type": "code",
        "outputId": "155a5083-4a98-4b00-d8bb-2687f4d5ed0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dir = '/content/Output/Test'\n",
        "create_dataset(dir,'/content/TestLabels.csv','/content/Test')\n",
        "dir = '/content/Output/Validation'\n",
        "create_dataset(dir,'/content/ValidationLabels.csv','/content/Validation')\n",
        "dir = '/content/Output/Train'\n",
        "create_dataset(dir,'/content/TrainLabels.csv','/content/Train')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58bb7933e7d34d0a981c9cb93835d9f6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1784), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "975ad53f4dd241b2ad8bb0b475901acc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1429), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5eb780f7883745feb06ed0c817998555",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=5358), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIPKWcMwT6vO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "from tqdm.autonotebook import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import cv2\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYFWY14OXRKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class video_dataset(Dataset):\n",
        "  def __init__(self,pt):\n",
        "    self.pt = torch.load(pt)\n",
        "    self.key = list(self.pt.keys())\n",
        "  def __len__(self):\n",
        "    return len(self.key)\n",
        "  def __getitem__(self,idx):\n",
        "    k = self.key[idx]\n",
        "    data,label = self.pt[k]\n",
        "    return data,label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayFpzFh1ZJvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = video_dataset('/content/Train.pt')\n",
        "train_loader = DataLoader(train_data,batch_size = 64, num_workers = 4 , shuffle = True)\n",
        "valid_data = video_dataset('/content/Validation.pt')\n",
        "valid_loader = DataLoader(valid_data,batch_size = 64 , num_workers = 4 )\n",
        "test_data = video_dataset('/content/Test.pt')\n",
        "test_loader = DataLoader(test_data,batch_size = 64, num_workers = 4 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBCHlsxlDjzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.lstm = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True,dropout  = 0.3, bidirectional = True)\n",
        "        self.fc = nn.Linear(2*hidden_dim, 128)\n",
        "        self.fc1 = nn.Linear(128,output_dim)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "    def forward(self, x):\n",
        "        self.lstm.flatten_parameters()\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = self.relu(self.fc(out[:,-1,:]))\n",
        "        out = self.fc1(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaEq-8fJa7Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTMModel(89,256,2,4).cuda()\n",
        "learning_rate = 0.005\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikmbs075EIaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, path, header):\n",
        "        self.log_file = open(path, 'w')\n",
        "        self.logger = csv.writer(self.log_file, delimiter='\\t')\n",
        "        self.logger.writerow(header)\n",
        "        self.header = header\n",
        "\n",
        "    def __del(self):\n",
        "        self.log_file.close()\n",
        "\n",
        "    def log(self, values):\n",
        "        write_values = []\n",
        "        for col in self.header:\n",
        "            assert col in values\n",
        "            write_values.append(values[col])\n",
        "\n",
        "        self.logger.writerow(write_values)\n",
        "        self.log_file.flush()\n",
        "\n",
        "\n",
        "def load_value_file(file_path):\n",
        "    with open(file_path, 'r') as input_file:\n",
        "        value = float(input_file.read().rstrip('\\n\\r'))\n",
        "    return value\n",
        "\n",
        "\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    batch_size = targets.size(0)\n",
        "\n",
        "    _, pred = outputs.topk(1, 1, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(targets.view(1, -1))\n",
        "    n_correct_elems = correct.float().sum().item()\n",
        "    return 100* n_correct_elems / batch_size\n",
        "def update_lr(optimizer, lr):\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = lr\n",
        "def update_mom(optimizer, mom):\n",
        "    for g in optimizer.param_groups:\n",
        "        g['momentum'] = mom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVDuncBoqeo7",
        "colab_type": "code",
        "outputId": "a1108c7b-a3ea-40f5-a3e5-4648442858bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_epochs = 20\n",
        "!mkdir results\n",
        "arch = 'LSTM'\n",
        "result_path = os.path.join('./results',arch)\n",
        "os.makedirs(result_path,exist_ok = True)\n",
        "train_logger = Logger(\n",
        "            os.path.join(result_path, 'train{}.log'.format(arch)),\n",
        "            ['epoch', 'loss', 'acc', 'lr'])\n",
        "train_batch_logger = Logger(\n",
        "            os.path.join(result_path, 'train_batch{}.log'.format(arch)),\n",
        "            ['epoch', 'batch', 'iter', 'loss', 'acc', 'lr'])    \n",
        "val_logger = Logger(\n",
        "            os.path.join(result_path, 'val{}.log'.format(arch)), ['epoch', 'loss', 'acc'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘results’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVIogQQubc6M",
        "colab_type": "code",
        "outputId": "f5d13f7a-131e-4a1b-bb98-735aeb339c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import os\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer, epoch_logger, batch_logger, batch_size , result_path):\n",
        "    print('Training Epoch {}'.format(epoch))\n",
        "    model.train()\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    start_time = time.time()\n",
        "    end_time = time.time()\n",
        "    for i, (inputs, targets) in enumerate(data_loader):\n",
        "        data_time.update(time.time() - end_time)\n",
        "        inputs = inputs[:,::2,:]\n",
        "        if torch.cuda.is_available():\n",
        "            targets = targets.cuda()\n",
        "            inputs = inputs.cuda()\n",
        "        outputs = model(inputs.type(torch.cuda.FloatTensor))\n",
        "        loss = criterion(outputs, targets)\n",
        "        acc = calculate_accuracy(outputs, targets)\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        accuracies.update(acc, inputs.size(0))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        del inputs,targets,outputs\n",
        "        batch_time.update(time.time() - end_time)\n",
        "        end_time = time.time()\n",
        "\n",
        "        batch_logger.log({\n",
        "            'epoch': epoch,\n",
        "            'batch': i + 1,\n",
        "            'iter': (epoch - 1) * len(data_loader) + (i + 1),\n",
        "            'loss': losses.val,\n",
        "            'acc': accuracies.val,\n",
        "            'lr': optimizer.param_groups[0]['lr']\n",
        "        })\n",
        "        sys.stdout.write(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d / %d] [Time %.2f %.2f] [Data %.2f %.2f] [Loss: %f, Acc: %.2f%%]\"\n",
        "                % (\n",
        "                    epoch,\n",
        "                    num_epochs,\n",
        "                    i,\n",
        "                    len(data_loader),\n",
        "                    batch_time.val,\n",
        "                    batch_time.avg,\n",
        "                    data_time.val,\n",
        "                    data_time.avg,\n",
        "                    losses.avg,\n",
        "                    accuracies.avg\n",
        "                    )\n",
        "                )\n",
        "    print('\\nEpoch time {} mins'.format((end_time-start_time)/60))\n",
        "    epoch_logger.log({\n",
        "        'epoch': epoch,\n",
        "        'loss': losses.avg,\n",
        "        'acc': accuracies.avg,\n",
        "        'lr': optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "        \n",
        "    save_file_path = os.path.join(result_path,'save.pth')\n",
        "    states = {\n",
        "        'epoch': epoch + 1,\n",
        "        'arch': arch,\n",
        "        'state_dict': model.state_dict()\n",
        "    }\n",
        "    torch.save(states, save_file_path)\n",
        "def val_epoch(epoch, num_epochs,data_loader, model, criterion,  logger , batch_size):\n",
        "    print('Validation Epoch {}'.format(epoch))\n",
        "    model.eval()\n",
        "    \n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "\n",
        "    end_time = time.time()\n",
        "    for i, (inputs, targets) in enumerate(data_loader):\n",
        "        data_time.update(time.time() - end_time)\n",
        "        if torch.cuda.is_available():\n",
        "            targets = targets.cuda()\n",
        "            inputs = inputs.cuda()\n",
        "        inputs = inputs[:,::2,:]\n",
        "        inputs = Variable(inputs)\n",
        "        targets = Variable(targets)\n",
        "        outputs = model(inputs.type(torch.cuda.FloatTensor))\n",
        "        loss = criterion(outputs, targets)\n",
        "        acc = calculate_accuracy(outputs, targets)\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        accuracies.update(acc, inputs.size(0))\n",
        "        batch_time.update(time.time() - end_time)\n",
        "        end_time = time.time()\n",
        "        del targets,inputs\n",
        "        sys.stdout.write(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d / %d] [Time %.2f %.2f] [Data %.2f %.2f] [Loss: %f, Acc: %.2f%%]\"\n",
        "                % (\n",
        "                    epoch,\n",
        "                    num_epochs,\n",
        "                    i,\n",
        "                    len(data_loader),\n",
        "                    batch_time.val,\n",
        "                    batch_time.avg,\n",
        "                    data_time.val,\n",
        "                    data_time.avg,\n",
        "                    losses.avg,\n",
        "                    accuracies.avg\n",
        "                    )\n",
        "                )\n",
        "    print('')\n",
        "    logger.log({'epoch': epoch, 'loss': losses.avg, 'acc': accuracies.avg})\n",
        "    return losses.avg\n",
        "def test(model, data_loader ,criterion, batch_size, logger = None):\n",
        "    print('Testing')\n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    for i, (inputs, targets) in enumerate(data_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            targets = targets.cuda()\n",
        "            inputs = inputs.cuda()\n",
        "        inputs = Variable(inputs)\n",
        "        inputs = inputs[:,::2,:]\n",
        "        targets = Variable(targets)\n",
        "        outputs = model(inputs.type(torch.cuda.FloatTensor))\n",
        "        loss = criterion(outputs, targets)\n",
        "        acc = calculate_accuracy(outputs, targets)\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        accuracies.update(acc, inputs.size(0))\n",
        "        sys.stdout.write(\n",
        "                \"\\r[Batch %d / %d]  [Loss: %f, Acc: %.2f%%]\"\n",
        "                % (\n",
        "                    i,\n",
        "                    len(data_loader),\n",
        "                    losses.avg,\n",
        "                    accuracies.avg\n",
        "                    )\n",
        "                )\n",
        "    if(logger):\n",
        "        logger.log({'epoch': epoch, 'loss': losses.avg, 'acc': accuracies.avg})\n",
        "    print('\\nAccuracy {}'.format(accuracies.avg))\n",
        "    return losses.avg,accuracies.avg\n",
        "for epoch in range(1,num_epochs):\n",
        "    train_epoch(epoch, num_epochs,train_loader, model, criterion, optimizer, train_logger, train_batch_logger , 64 , result_path)\n",
        "    val_epoch(epoch, num_epochs, valid_loader, model, criterion, val_logger , 64)\n",
        "    test(model, test_loader, criterion , 64)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c74e653c6b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_logger\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mresult_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_logger\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-c74e653c6b12>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, num_epochs, data_loader, model, criterion, optimizer, epoch_logger, batch_logger, batch_size, result_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-35761da99372>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ]
}