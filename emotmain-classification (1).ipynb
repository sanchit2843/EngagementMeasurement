{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modular_code.utils import *\n",
    "#from Modular_code.model import Model\n",
    "#from Modular_code.train import train_epoch\n",
    "#from Modular_code.val import val_epoch\n",
    "from Modular_code.opts import *\n",
    "from Modular_code.clr import OneCycle\n",
    "from Modular_code.radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_label = pd.read_csv(train_label_path)\n",
    "valid_label = pd.read_csv(valid_label_path)\n",
    "test_label = pd.read_csv(test_label_path)\n",
    "train_id,train_classes = create_label(train_label,'./Output/Train') \n",
    "test_id,test_classes = create_label(test_label,'./Output/Test') \n",
    "valid_id,valid_classes = create_label(valid_label,'./Output/Validation') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def change_label(eng,bore,frus,conf):\n",
    "    if(eng == 1 or eng==2):\n",
    "        if(bore >= 2 or frus >= 2 or conf >= 2):\n",
    "            return 0\n",
    "        return 1\n",
    "    if(eng == 3):\n",
    "        if(bore >= 2 or frus >= 2 or conf >= 2):\n",
    "            return 0\n",
    "        return 2\n",
    "    if(bore >= 2 or frus >= 2 or conf >= 2 or eng <= 2):\n",
    "        return 0\n",
    "def create_label(data_file,path):\n",
    "    id_1 = data_file['ClipID']\n",
    "    label = data_file['Engagement']\n",
    "    bore = data_file.Boredom.values\n",
    "    conf = data_file.Confusion.values\n",
    "    frus = data_file.iloc[:,4].values\n",
    "    id_val = []\n",
    "    label_val = []\n",
    "    for id in range(len(id_1)):\n",
    "        path1 = os.path.join(path,id_1[id][:6])\n",
    "        id_2 = id_1[id][:-4]\n",
    "        video_path = os.path.join(path1,id_2)\n",
    "        if(os.path.exists(video_path)):\n",
    "            try:\n",
    "                p = os.path.join(video_path,os.listdir(video_path)[0])\n",
    "                if(os.path.exists(p)):\n",
    "                    cap = cv2.VideoCapture(p)\n",
    "                    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                    if(length>=300):\n",
    "                        id_val.append(os.path.join(video_path,os.listdir(video_path)[0]))\n",
    "                        label_val.append(change_label(label[id],bore[id],frus[id],conf[id]))\n",
    "            except:\n",
    "                pass\n",
    "    return id_val,label_val\n",
    "\n",
    "train_id,train_classes = create_label(train_label,'./Output/Train') \n",
    "test_id,test_classes = create_label(test_label,'./Output/Test') \n",
    "valid_id,valid_classes = create_label(valid_label,'./Output/Validation') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label(id,classes):\n",
    "    id_new0 = []\n",
    "    id_new1 = []\n",
    "    for i,j in enumerate(classes):\n",
    "        if(j == 0):\n",
    "            id_new0.append(id[i])\n",
    "        if(j == 1):\n",
    "            id_new1.append(id[i])\n",
    "    return id_new0,id_new1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = train_id+test_id\n",
    "train_classes = train_classes+test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id0,train_id1 = extract_label(train_id,train_classes)\n",
    "import numpy as np\n",
    "train_id = train_id + 45*train_id0 + 7*train_id1\n",
    "train_classes = train_classes + np.zeros(len(45*train_id0),dtype = int).tolist() + np.ones(len(7*train_id1) , dtype = int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "print(Counter(valid_classes).keys()) # equals to list(set(words))\n",
    "np.asarray(list(Counter(train_classes).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "im_size = 112\n",
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n",
    "class video_dataset(Dataset):\n",
    "    def __init__(self,id,label,sequence_length = 60,transform = None,transform1 = None):\n",
    "        self.id =id\n",
    "        self.engagement = label\n",
    "        self.transform = transform\n",
    "        self.transform1 = transform1\n",
    "        self.count = sequence_length\n",
    "    def __len__(self):\n",
    "        return len(self.id)\n",
    "    def __getitem__(self,idx):\n",
    "        video_path = self.id[idx]\n",
    "        frames = []\n",
    "        a = int(300/self.count)\n",
    "        hp = np.random.randint(0,2)\n",
    "        aug = np.random.randint(0,2)            \n",
    "        id1 = np.random.randint(0,a)\n",
    "        label = self.engagement[idx]\n",
    "        for i,frame in enumerate(frame_extract(video_path)):\n",
    "            if(i % a == id1):\n",
    "                if(label < 2):\n",
    "                    if(hp == 1):\n",
    "                        frame = cv2.flip(frame,1)\n",
    "                frames.append(frame.transpose(2,0,1))\n",
    "        if(aug == 1):\n",
    "            if(self.transform1):\n",
    "                aug_clas = np.random.randint(0,len(self.transform1))\n",
    "                frames = list(self.transform1[aug_clas].augment_batches(frames,background = False))\n",
    "        for idx,i in enumerate(frames):\n",
    "            frames[idx] = self.transform(i.transpose(1,2,0))\n",
    "        frames = torch.stack(frames)\n",
    "        #frames = frames[::a,:,:,:]\n",
    "        frames = frames[:self.count]\n",
    "#        frames = frames.permute(1,0,2,3)\n",
    "#        label = label/3\n",
    "#        print(np.asarray(label)\n",
    "#        label = one_hot(np.asarray(label),4)\n",
    "        return frames,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 112\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "sometimes = lambda aug: iaa.Sometimes(1, aug)\n",
    "seq = [\n",
    "    \n",
    "                    iaa.GaussianBlur((0, 1.0)),\n",
    "                    iaa.MedianBlur(k=(3, 5)),\n",
    "                    iaa.MotionBlur(k=3),\n",
    "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                \n",
    "]\n",
    "train_transforms = transforms.Compose([\n",
    "                                        transforms.ToPILImage(),\n",
    "                                        transforms.Resize((im_size,im_size)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean,std)])\n",
    "train_data = video_dataset(train_id,train_classes,sequence_length = 100,transform = train_transforms)\n",
    "val_data = video_dataset(valid_id,valid_classes,sequence_length = 100,transform = train_transforms)\n",
    "#test_data = video_dataset(test_id,test_classes,sequence_length = 100,transform = train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "image,label = train_data[300]\n",
    "#frame = augment(image[10,:,:,:].permute(1,2,0).numpy(),1)\n",
    "#frame = torch.from_numpy(frame)\n",
    "print(label)\n",
    "\n",
    "#for i in range(100):\n",
    "im_plot(image[50,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConvLSTM_pytorch.convlstm import ConvLSTM\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        model = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "        model._fc = nn.Linear(1792,7)\n",
    "        model.load_state_dict(torch.load('./effb4emotion.pt'))\n",
    "        model._fc = Identity()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.model1 = model\n",
    "        c1 = ConvLSTM(  input_size=(size,size),\n",
    "                             input_dim= 1792,\n",
    "                             hidden_dim=[128,512],\n",
    "                             kernel_size=(3,3),\n",
    "                             num_layers=2,\n",
    "                             batch_first = True,\n",
    "                             bias= True,\n",
    "                             return_all_layers = False)\n",
    "        self.c1 = c1\n",
    "        self.f1 = nn.Linear(512*size*size,2048)\n",
    "        self.relu = Swish()\n",
    "        self.fc2 = nn.Linear(2048,512)\n",
    "        self.fc3 = nn.Linear(512,2)\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "    def forward(self, input):\n",
    "        x = self.model1.extract_features(input)\n",
    "        x = x.view(-1,100,1792,size,size).detach()\n",
    "        x = self.c1(x)\n",
    "        x = torch.mean(x[0][0],dim = 1)\n",
    "        x = x.view(-1,512*size*size).detach()\n",
    "        x = self.dp(self.relu(self.f1(x)))\n",
    "        x = self.dp(self.relu(self.fc2(x)))\n",
    "        x = self.dp(self.fc3(x))\n",
    "        return x\n",
    "#model = Model().to('cuda')\n",
    "#model = nn.DataParallel(model)\n",
    "#model.load_state_dict(torch.load('./results/emotionnet-100-bin-convlstm-effb4-112/save.pth')['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models.resnet as resnet\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "class SwishImplementation(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = torch.sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return SwishImplementation.apply(x)\n",
    "\n",
    "#         Encoder\n",
    "##############################\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "#        block = resnet.BasicBlock\n",
    "#        num_classes = 7\n",
    "#        layers = [3,4,23,3]\n",
    "#        model = resnet.ResNet(block, layers, num_classes)\n",
    "#        model.load_state_dict(torch.load('./checkpoint.pth.tar'))\n",
    "        model = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "        model._fc = nn.Linear(1792,7)\n",
    "        model.load_state_dict(torch.load('./effb4emotion.pt'))\n",
    "        model._fc = Identity()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "#        model._fc = nn.Linear(1792,7)\n",
    "#        model.load_state_dict(torch.load('./effb4emotion.pt'))\n",
    "        self.model = model\n",
    "        self.bn1 = nn.BatchNorm1d(2048, momentum=0.01)\n",
    "        #self.final = nn.Sequential(nn.Linear(1280, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01))        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "       # x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "##############################\n",
    "#           LSTM\n",
    "##############################\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, latent_dim, num_layers, hidden_dim, bidirectional):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional,dropout = 0.3)\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_state = torch.randn(150, 2, 1024)\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        x,_ = self.lstm(x, None)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models.resnet as resnet\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "block = resnet.BasicBlock\n",
    "num_classes = 7\n",
    "layers = [3,4,23,3]\n",
    "model = resnet.ResNet(block, layers, num_classes)\n",
    "model.load_state_dict(torch.load('./checkpoint.pth.tar'))\n",
    "\n",
    "class SwishImplementation(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = torch.sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return SwishImplementation.apply(x)\n",
    "\n",
    "#         Encoder\n",
    "##############################\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        block = resnet.BasicBlock\n",
    "        num_classes = 7\n",
    "        layers = [3,4,23,3]\n",
    "        model = resnet.ResNet(block, layers, num_classes)\n",
    "        model.load_state_dict(torch.load('./checkpoint.pth.tar'))\n",
    "        model = models.resnext50_32x4d(pretrained=True)\n",
    "        model.fc = nn.Linear(2048,3)\n",
    "        model.load_state_dict(torch.load('./WACV/best.pth'))\n",
    "        model = nn.Sequential(*list(model.children())[:-2])\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "#        model._fc = nn.Linear(1792,7)\n",
    "#        model.load_state_dict(torch.load('./effb4emotion.pt'))\n",
    "        self.model = model\n",
    "        self.bn1 = nn.BatchNorm1d(2048, momentum=0.01)\n",
    "        #self.final = nn.Sequential(nn.Linear(1280, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01))        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "       # x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "##############################\n",
    "#           LSTM\n",
    "##############################\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, latent_dim, num_layers, hidden_dim, bidirectional):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional,dropout = 0.3)\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_state = torch.randn(150, 2, 1024)\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        x,_ = self.lstm(x, None)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class FAN(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 1024, bidirectional = True):\n",
    "        super(FAN, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.relu = nn.ELU()\n",
    "        self.attention1 = nn.Linear(latent_dim,1)\n",
    "        self.attention2 = nn.Linear(latent_dim*2,1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size = 1)\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        c1 = ConvLSTM(  input_size=(size,size),\n",
    "                     input_dim= 2048,\n",
    "                     hidden_dim=[128,256],\n",
    "                     kernel_size=(3,3),\n",
    "                     num_layers=2,\n",
    "                     batch_first = True,\n",
    "                     bias= True,\n",
    "                     return_all_layers = False)\n",
    "        self.c1 = c1\n",
    "\n",
    "        self.output_layers1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim*2, 1024),nn.ELU(),nn.Dropout(0.4),nn.Linear(1024,4),nn.Dropout(0.4))\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        x1 = self.encoder(x)\n",
    "        x = self.pool(x1)\n",
    "        x = x.view(batch_size,seq_length,2048)\n",
    "        v1 = []\n",
    "        for i in range(x.shape[0]):\n",
    "            alpha = self.sig(self.attention1(self.dp(x[i])))\n",
    "            x1 = torch.sum(x[i]*alpha,dim = 0)\n",
    "            a1 = torch.sum(alpha , dim = 0)\n",
    "            x1 = torch.div(x1,a1)\n",
    "            x11 = x1.unsqueeze(dim = 0).repeat(seq_length,1)\n",
    "            x3 = torch.cat([x[i],x11],dim = 1)\n",
    "            betas = self.sig(self.attention2(self.dp(x3)))\n",
    "            v1.append(x3*alpha*betas)\n",
    "        v1 = torch.stack(v1)\n",
    "        v1 = torch.sum(v1,dim = 1)\n",
    "        v1 = torch.div(v1,torch.sum(alpha*betas,dim = 0))\n",
    "        x = self.c1(x1)\n",
    "        x = x[0][0].view(-1,256*size*size)\n",
    "        #att = self.dp(self.sig(self.attention(x))).view(-1,100)\n",
    "        x = torch.mean(x.view(-1,100,256*size*size),dim = 1)\n",
    "        print(x.shape)\n",
    "        return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/sanjaymoto75/.conda/envs/env/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/sanjaymoto75/.conda/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"<ipython-input-109-cfa5d0a0062c>\", line 44, in forward\n    x = self.c1(x1)\n  File \"/home/sanjaymoto75/.conda/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/sanjaymoto75/data/home/sanjaymoto75/attention/ConvLSTM_pytorch/convlstm.py\", line 128, in forward\n    seq_len = input_tensor.size(1)\nIndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-4467d5a535f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#model_dict.update(pretrained_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/sanjaymoto75/.conda/envs/env/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/sanjaymoto75/.conda/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"<ipython-input-109-cfa5d0a0062c>\", line 44, in forward\n    x = self.c1(x1)\n  File \"/home/sanjaymoto75/.conda/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/sanjaymoto75/data/home/sanjaymoto75/attention/ConvLSTM_pytorch/convlstm.py\", line 128, in forward\n    seq_len = input_tensor.size(1)\nIndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "model = FAN(4).cuda()\n",
    "model = nn.DataParallel(model)\n",
    "#model_dict = model.state_dict()\n",
    "#pretrained_dict = torch.load('./results/face-100-comp-fan/save.pth')['state_dict']\n",
    "#pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "#model_dict.update(pretrained_dict)\n",
    "model(image.unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-0222d3388597>, line 85)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-0222d3388597>\"\u001b[0;36m, line \u001b[0;32m85\u001b[0m\n\u001b[0;31m    x =\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "size = 3\n",
    "from torchvision import models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "class SwishImplementation(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "        sigmoid_i = torch.sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return SwishImplementation.apply(x)\n",
    "from ConvLSTM_pytorch.convlstm import ConvLSTM\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        model = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "        model._fc = nn.Linear(1792,7)\n",
    "        model.load_state_dict(torch.load('./effb4emotion.pt'))\n",
    "        model._fc = Identity()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.model1 = model\n",
    "        c1 = ConvLSTM(  input_size=(size,size),\n",
    "                             input_dim= 1792,\n",
    "                             hidden_dim=[128,256],\n",
    "                             kernel_size=(3,3),\n",
    "                             num_layers=2,\n",
    "                             batch_first = True,\n",
    "                             bias= True,\n",
    "                             return_all_layers = False)\n",
    "        self.c1 = c1\n",
    "#        self.attention = nn.Linear(256*size*size,1)\n",
    "        self.f1 = nn.Linear(256*size*size+256,512)\n",
    "        self.relu = nn.ReLU()\n",
    "#        self.fc2 = nn.Linear(2048,512)\n",
    "        self.att_red = nn.Linear(1792,256)\n",
    "        self.fc4 = nn.Linear(512,4)\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size = 1)\n",
    "        self.attention1 = nn.Linear(1792,1)\n",
    "#        self.beta_attention = nn.Linear(4096,1)\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1,3,112,112)\n",
    "        x = self.model1.extract_features(input)\n",
    "        x_pool = self.pool(x).reshape(-1,100,1792)\n",
    "        x = x.view(-1,100,1792,size,size)\n",
    "        x = self.c1(x)\n",
    "        x = x[0][0].view(-1,256*size*size)\n",
    "        x = x.view(-1,100,256*size*size)\n",
    "        #x1 = []\n",
    "        x2 = []\n",
    "        v1 = []\n",
    "        for i in range(x_pool.shape[0]):            \n",
    "            alpha = self.sig(self.attention1(self.dp(x_pool[i])))\n",
    "            #x1.append(torch.div(torch.sum(x[i]*att[i].reshape(-1,1),dim = 0),torch.sum(att[i])))\n",
    "            x2.append(torch.div(torch.sum(x_pool[i]*alpha.reshape(-1,1),dim = 0),torch.sum(alpha,dim = 0)))\n",
    "        x2 = torch.stack(x2)\n",
    "        x2 = self.relu(self.att_red(x2))\n",
    "#        v1 = torch.stack(v1)\n",
    "#        v1 = torch.sum(v1,dim = 1)\n",
    "#        v1 = torch.div(v1,torch.sum(alpha*betas,dim = 0))\n",
    "        x1 = torch.mean(x,dim = 1)\n",
    "        x = torch.cat([x1,x2],dim = 1)\n",
    "#        x = self.dp(self.relu(self.fc3(x)))\n",
    "        #x = torch.div(x,torch.sum(att,dim = 1))\n",
    "        x = self.dp(self.relu(self.f1(x)))\n",
    "#        x = self.dp(self.relu(self.fc2(x)))\n",
    "        x = self.dp(self.fc4(x))\n",
    "        x = \n",
    "        return x\n",
    "model = Model().to('cuda')\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 4\n",
    "from torchvision import models\n",
    "from ConvLSTM_pytorch.convlstm import ConvLSTM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        model = models.resnext50_32x4d(pretrained=True)\n",
    "        model.fc = nn.Linear(2048,3)\n",
    "        model.load_state_dict(torch.load('./WACV/best.pth'))\n",
    "        model1 = nn.Sequential(*list(model.children())[:-2])\n",
    "        for param in model1.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.model1 = model1\n",
    "        c1 = ConvLSTM(  input_size=(size,size),\n",
    "                             input_dim= 2048,\n",
    "                             hidden_dim=[128,256],\n",
    "                             kernel_size=(3,3),\n",
    "                             num_layers=2,\n",
    "                             batch_first = True,\n",
    "                             bias= True,\n",
    "                             return_all_layers = False)\n",
    "        self.c1 = c1\n",
    "#        self.f1 = nn.Linear(256*size*size,512)\n",
    "        self.f1 = nn.Linear(256*size*size+512,512)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.att_red = nn.Linear(2048,512)\n",
    "        self.fc4 = nn.Linear(512,4)\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size = 1)\n",
    "        self.attention1 = nn.Linear(2048,1)        \n",
    "        self.r = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1,3,112,112)\n",
    "        x = self.model1(input)\n",
    "        x_pool = self.relu(self.pool(x)).reshape(-1,100,2048)\n",
    "        x = x.view(-1,100,2048,size,size)\n",
    "        x = self.c1(x)\n",
    "        x = x[0][0].view(-1,256*size*size)\n",
    "        x = x.view(-1,100,256*size*size)\n",
    "        x2 = []\n",
    "        v1 = []\n",
    "        for i in range(x_pool.shape[0]):            \n",
    "            alpha = self.sig(self.attention1(self.dp(x_pool[i])))\n",
    "#            x1.append(torch.div(torch.sum(x[i]*att[i].reshape(-1,1),dim = 0),torch.sum(att[i])))\n",
    "            x2.append(torch.div(torch.sum(x_pool[i]*alpha.reshape(-1,1),dim = 0),torch.sum(alpha,dim = 0)))\n",
    "        x2 = torch.stack(x2)\n",
    "        x2 = self.relu(self.att_red(x2))\n",
    "        x = torch.mean(x,dim = 1)\n",
    "        x = torch.cat([x,x2],dim = 1)\n",
    "        x1 = self.dp(self.relu(self.f1(x)))\n",
    "        x = self.dp(self.fc4(x1))\n",
    "        return x\n",
    "model = Model().to('cuda')\n",
    "#model = nn.DataParallel(model)\n",
    "#model.load_state_dict(torch.load('./results/face-wacv-100-comp-convlstm+att/save.pth')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0161, 0.0544, 0.0058],\n",
       "        [0.0685, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
       "       grad_fn=<FusedDropoutBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "im_size = 112\n",
    "model(torch.from_numpy(np.empty((2,100,3,im_size,im_size))).type(torch.cuda.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'face-wacv-100-comp-convlstm+att-1'\n",
    "result_path = os.path.join('./results',arch)\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "os.makedirs(result_path,exist_ok = True)\n",
    "train_logger = Logger(\n",
    "            os.path.join(result_path, 'train{}.log'.format(arch)),\n",
    "            ['epoch', 'loss', 'acc', 'lr'])\n",
    "train_batch_logger = Logger(\n",
    "            os.path.join(result_path, 'train_batch{}.log'.format(arch)),\n",
    "            ['epoch', 'batch', 'iter', 'loss', 'acc', 'lr'])    \n",
    "val_logger = Logger(\n",
    "            os.path.join(result_path, 'val{}.log'.format(arch)), ['epoch', 'loss', 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(optimizer, lr):\n",
    "    for group in optimizer.param_groups:\n",
    "        group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer, epoch_logger, batch_logger, batch_size , onecyc , writer , result_path):\n",
    "    print('Training Epoch {}'.format(epoch))\n",
    "    model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "    start_time = time.time()\n",
    "    end_time = time.time()\n",
    "    t = []\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        if(targets.shape[0]<batch_size):\n",
    "            continue\n",
    "        data_time.update(time.time() - end_time)\n",
    "        if torch.cuda.is_available():\n",
    "            targets = targets.type(torch.cuda.LongTensor)\n",
    "            inputs = inputs.cuda()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "#        targets = torch.nn.functional.one_hot(targets,4)\n",
    "#        print(targets)\n",
    "        loss = torch.mean(criterion(outputs, targets))\n",
    "#        _,targets = torch.max(targets,1)\n",
    "        acc = calculate_accuracy(outputs, targets)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        accuracies.update(acc, inputs.size(0))\n",
    "        lr,_ = onecyc.calc()\n",
    "        update_lr(optimizer, lr)\n",
    "        optimizer.zero_grad()\n",
    "#        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "#            scaled_loss.backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t.extend(list(targets.detach().cpu().numpy()))\n",
    "        del inputs,targets,outputs\n",
    "        batch_time.update(time.time() - end_time)\n",
    "        end_time = time.time()\n",
    "        batch_logger.log({\n",
    "            'epoch': epoch,\n",
    "            'batch': i + 1,\n",
    "            'iter': (epoch - 1) * len(data_loader) + (i + 1),\n",
    "            'loss': losses.val,\n",
    "            'acc': accuracies.val,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "        writer.add_scalar('data/acc', accuracies.val, (epoch - 1) * len(data_loader) + (i + 1))\n",
    "        writer.add_scalar('data/loss', losses.val, (epoch - 1) * len(data_loader) + (i + 1))        \n",
    "        sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d / %d] [Time %.2f %.2f] [Data %.2f %.2f] [Loss: %f, Acc: %.2f%%]\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    i,\n",
    "                    len(data_loader),\n",
    "                    batch_time.val,\n",
    "                    batch_time.avg,\n",
    "                    data_time.val,\n",
    "                    data_time.avg,\n",
    "                    losses.avg,\n",
    "                    accuracies.avg\n",
    "                    )\n",
    "                )\n",
    "    print('\\nEpoch time {} mins'.format((end_time-start_time)/60))\n",
    "    epoch_logger.log({\n",
    "        'epoch': epoch,\n",
    "        'loss': losses.avg,\n",
    "        'acc': accuracies.avg,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "    writer.add_scalar('data/acc_epoch', accuracies.avg, epoch)\n",
    "    writer.add_scalar('data/loss_epoch', losses.avg, epoch)        \n",
    "        \n",
    "    save_file_path = os.path.join(result_path,'save.pth')\n",
    "    states = {\n",
    "        'state_dict': model.state_dict()\n",
    "    }\n",
    "    torch.save(states, save_file_path)\n",
    "def test(epoch,model, data_loader ,criterion, batch_size, result_path,best_acc = 0 , logger = None ):\n",
    "    print('Testing')\n",
    "    if(epoch == 1):\n",
    "        best_acc = 0\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "    pred = []\n",
    "    true = []\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            targets = targets.cuda()\n",
    "            inputs = inputs.cuda()\n",
    "        outputs = model(inputs)\n",
    "        _,p = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, targets)\n",
    "#        _,targets = torch.max(targets,1)\n",
    "        acc = calculate_accuracy(outputs, targets)\n",
    "        true += (targets.type(torch.cuda.LongTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n",
    "        pred += p.detach().cpu().numpy().reshape(len(p)).tolist()\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        accuracies.update(acc, inputs.size(0))\n",
    "        sys.stdout.write(\n",
    "                \"\\r[Batch %d / %d]  [Loss: %f, Acc: %.2f%%]\"\n",
    "                % (\n",
    "                    i,\n",
    "                    len(data_loader),\n",
    "                    losses.avg,\n",
    "                    accuracies.avg\n",
    "                    )\n",
    "                )\n",
    "    if(logger):\n",
    "        logger.log({'epoch': epoch, 'loss': losses.avg, 'acc': accuracies.avg})\n",
    "    print('\\nAccuracy {}'.format(accuracies.avg))\n",
    "    if(accuracies.avg>best_acc):\n",
    "        best_acc = accuracies.avg\n",
    "        result_path = os.path.join(result_path,'best.pth')\n",
    "        state = {\n",
    "        'acc':best_acc,\n",
    "         'state':model.state_dict()\n",
    "        }\n",
    "        torch.save(state,result_path)\n",
    "    return true,pred,best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class WeightedKappaLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, preds, true,nb_classes = None):\n",
    "        #step1 confusion matrix\n",
    "        nb_classes = preds.shape[1]\n",
    "        _,pred = torch.max(preds,1)\n",
    "        confusion_matrix = torch.empty([nb_classes, nb_classes],requires_grad = True)\n",
    "        confusion_matrix.requires_grad = True\n",
    "        for t, p in zip(pred.view(-1), true.view(-1)):\n",
    "            confusion_matrix[p.long(), t.long()] += 1\n",
    "        weights = torch.empty([nb_classes,nb_classes],requires_grad = True)\n",
    "        weights.requires_grad = True\n",
    "        #weight matrix\n",
    "        for i in range(len(weights)):\n",
    "            for j in range(len(weights)):\n",
    "                \n",
    "                weights[i][j] = float(((i-j)**2)/(len(weights)-1)**2)\n",
    "        #Histograms\n",
    "        true_hist= torch.empty([nb_classes],requires_grad = True)\n",
    "        true_hist.requires_grad = True\n",
    "        for item in true: \n",
    "            true_hist[item]+=1\n",
    "        pred_hist=torch.empty([nb_classes],requires_grad = True)\n",
    "        pred_hist.requires_grad = True\n",
    "        for item in pred: \n",
    "            pred_hist[int(item)]+=1\n",
    "        E = torch.ger(true_hist,pred_hist)\n",
    "        E = E/E.sum()\n",
    "        confusion_matrix = confusion_matrix/confusion_matrix.sum()\n",
    "        num = (confusion_matrix*weights).sum()\n",
    "        den = (E*weights).sum()\n",
    "        return num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjaymoto75/.conda/envs/env/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0417, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class sanchitLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sig = nn.Tanh()\n",
    "        self.soft = nn.LogSoftmax()\n",
    "    def forward(self, pred, true):\n",
    "        clases = torch.arange(0,pred.shape[1],1).reshape(1,-1).repeat(pred.shape[0],1)\n",
    "        if(torch.cuda.is_available()):\n",
    "            clases = clases.cuda()\n",
    "        clases = (abs(clases-true.reshape(-1,1))).type(torch.cuda.DoubleTensor)\n",
    "        clases = clases/(clases.shape[1]-1)\n",
    "        if(torch.cuda.is_available()):\n",
    "            dis = torch.log(torch.cosh(clases+1e-12)).type(torch.cuda.DoubleTensor)\n",
    "            pred = self.soft(pred).type(torch.cuda.DoubleTensor)\n",
    "        else:\n",
    "            dis = torch.log(torch.cosh(clases+1e-12)).type(torch.cuda.DoubleTensor)\n",
    "            pred = self.soft(pred).type(torch.DoubleTensor)\n",
    "        return torch.mean(-1*torch.sum(dis * pred,axis = 1),axis = 0)\n",
    "s = sanchitLoss()\n",
    "s(torch.from_numpy(np.asarray([0.1,0.2,0.3,0.4])).unsqueeze(dim = 0).type(torch.cuda.FloatTensor),torch.from_numpy(np.asarray([3])).type(torch.cuda.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjaymoto75/.conda/envs/env/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4338, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "class sanchitLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sig = nn.Tanh()\n",
    "        self.soft1 = nn.LogSoftmax()\n",
    "        self.soft = nn.Softmax()\n",
    "        self.nll = nn.NLLLoss()\n",
    "    def forward(self, pred, true):\n",
    "        clases = torch.arange(0,pred.shape[1],1).reshape(1,-1).repeat(pred.shape[0],1)\n",
    "        if(torch.cuda.is_available()):\n",
    "            clases = clases.cuda()\n",
    "        clases = (abs(clases-true.reshape(-1,1))).type(torch.cuda.DoubleTensor)\n",
    "        dis = clases/(clases.shape[1]-1)\n",
    "        #print(self.nll(self.soft1(pred),true))\n",
    "        if(torch.cuda.is_available()):\n",
    "#            dis = self.sig(clases).type(torch.cuda.DoubleTensor)\n",
    "            pred = self.soft(pred).type(torch.cuda.DoubleTensor)\n",
    "        else:\n",
    "            #dis = self.sig(clases).type(torch.cuda.DoubleTensor)\n",
    "            pred = self.soft(pred).type(torch.DoubleTensor)\n",
    "        return torch.mean(torch.sum(dis * pred,axis = 1).type(torch.cuda.DoubleTensor),axis = 0)\n",
    "s = sanchitLoss()\n",
    "s(torch.from_numpy(np.asarray([0,0.2,0.3,0.5])).unsqueeze(dim = 0).type(torch.cuda.FloatTensor),torch.from_numpy(np.asarray([3])).type(torch.cuda.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1\n",
      "[Epoch 1/15] [Batch 168 / 170] [Time 1.37 1.58] [Data 0.12 0.18] [Loss: 1.789602, Acc: 30.13%]\n",
      "Epoch time 4.461353965600332 mins\n",
      "Testing\n",
      "[Batch 21 / 23]  [Loss: 1.537648, Acc: 18.75%]\n",
      "Accuracy 18.75\n",
      "Comb\n",
      " [[  3  19   0   1]\n",
      " [  7 123   9   4]\n",
      " [ 46 599  84  75]\n",
      " [ 57 279  48  54]]\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.03      0.13      0.92      0.04      0.35      0.11        23\n",
      "          1       0.12      0.86      0.29      0.21      0.50      0.26       143\n",
      "          2       0.60      0.10      0.91      0.18      0.31      0.09       804\n",
      "          3       0.40      0.12      0.92      0.19      0.34      0.10       438\n",
      "\n",
      "avg / total       0.48      0.19      0.85      0.18      0.34      0.11      1408\n",
      "\n",
      "Training Epoch 2\n",
      "[Epoch 2/15] [Batch 168 / 170] [Time 1.41 1.58] [Data 0.13 0.18] [Loss: 1.603441, Acc: 41.24%]\n",
      "Epoch time 4.462957179546356 mins\n",
      "Testing\n",
      "[Batch 21 / 23]  [Loss: 1.469799, Acc: 34.02%]\n",
      "Accuracy 34.01988636363637\n",
      "Comb\n",
      " [[  3  17   3   0]\n",
      " [  7 105  29   2]\n",
      " [ 50 387 311  56]\n",
      " [ 46 191 141  60]]\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.03      0.13      0.93      0.05      0.35      0.11        23\n",
      "          1       0.15      0.73      0.53      0.25      0.62      0.40       143\n",
      "          2       0.64      0.39      0.71      0.48      0.53      0.27       804\n",
      "          3       0.51      0.14      0.94      0.22      0.36      0.12       438\n",
      "\n",
      "avg / total       0.54      0.34      0.77      0.37      0.48      0.23      1408\n",
      "\n",
      "Training Epoch 3\n",
      "[Epoch 3/15] [Batch 168 / 170] [Time 1.39 1.59] [Data 0.14 0.18] [Loss: 1.487779, Acc: 44.79%]\n",
      "Epoch time 4.465512259801229 mins\n",
      "Testing\n",
      "[Batch 21 / 23]  [Loss: 1.455454, Acc: 37.00%]\n",
      "Accuracy 37.00284090909091\n",
      "Comb\n",
      " [[  0  19   4   0]\n",
      " [  0  95  41   7]\n",
      " [  0 376 353  75]\n",
      " [  0 206 159  73]]\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.00      0.00      1.00      0.00      0.00      0.00        23\n",
      "          1       0.14      0.66      0.52      0.23      0.59      0.35       143\n",
      "          2       0.63      0.44      0.66      0.52      0.54      0.28       804\n",
      "          3       0.47      0.17      0.92      0.25      0.39      0.14       438\n",
      "\n",
      "avg / total       0.52      0.37      0.73      0.40      0.49      0.24      1408\n",
      "\n",
      "Training Epoch 4\n",
      "[Epoch 4/15] [Batch 168 / 170] [Time 1.40 1.59] [Data 0.13 0.18] [Loss: 1.382744, Acc: 49.87%]\n",
      "Epoch time 4.464621984958649 mins\n",
      "Testing\n",
      "[Batch 21 / 23]  [Loss: 1.283674, Acc: 45.17%]\n",
      "Accuracy 45.17045454545455\n",
      "Comb\n",
      " [[  4  12   4   3]\n",
      " [ 15  45  47  36]\n",
      " [ 31 108 351 314]\n",
      " [ 25  42 135 236]]\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.05      0.17      0.95      0.08      0.41      0.15        23\n",
      "          1       0.22      0.31      0.87      0.26      0.52      0.26       143\n",
      "          2       0.65      0.44      0.69      0.52      0.55      0.29       804\n",
      "          3       0.40      0.54      0.64      0.46      0.59      0.34       438\n",
      "\n",
      "avg / total       0.52      0.45      0.70      0.47      0.56      0.30      1408\n",
      "\n",
      "Training Epoch 5\n",
      "[Epoch 5/15] [Batch 168 / 170] [Time 1.38 1.58] [Data 0.12 0.18] [Loss: 1.292398, Acc: 51.29%]\n",
      "Epoch time 4.457650363445282 mins\n",
      "Testing\n",
      "[Batch 21 / 23]  [Loss: 1.257010, Acc: 54.90%]\n",
      "Accuracy 54.90056818181818\n",
      "Comb\n",
      " [[  3   8  11   1]\n",
      " [ 16  41  82   4]\n",
      " [ 23  87 659  35]\n",
      " [ 28  37 303  70]]\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.04      0.13      0.95      0.06      0.35      0.11        23\n",
      "          1       0.24      0.29      0.90      0.26      0.51      0.24       143\n",
      "          2       0.62      0.82      0.34      0.71      0.53      0.30       804\n",
      "          3       0.64      0.16      0.96      0.26      0.39      0.14       438\n",
      "\n",
      "avg / total       0.58      0.55      0.60      0.51      0.48      0.24      1408\n",
      "\n",
      "Training Epoch 6\n",
      "[Epoch 6/15] [Batch 168 / 170] [Time 1.37 1.58] [Data 0.12 0.18] [Loss: 1.293036, Acc: 51.92%]\n",
      "Epoch time 4.449359985192617 mins\n",
      "Testing\n",
      "[Batch 21 / 23]  [Loss: 1.329677, Acc: 50.00%]\n",
      "Accuracy 50.0\n",
      "Comb\n",
      " [[  0  12   6   5]\n",
      " [  0  72  47  24]\n",
      " [  1 174 465 164]\n",
      " [  0  69 202 167]]\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.00      0.00      1.00      0.00      0.00      0.00        23\n",
      "          1       0.22      0.50      0.80      0.31      0.63      0.39       143\n",
      "          2       0.65      0.58      0.58      0.61      0.58      0.33       804\n",
      "          3       0.46      0.38      0.80      0.42      0.55      0.29       438\n",
      "\n",
      "avg / total       0.54      0.50      0.68      0.51      0.57      0.32      1408\n",
      "\n",
      "Training Epoch 7\n",
      "[Epoch 7/15] [Batch 168 / 170] [Time 1.37 1.58] [Data 0.13 0.18] [Loss: 1.238564, Acc: 53.75%]\n",
      "Epoch time 4.446755345662435 mins\n",
      "Testing\n",
      "[Batch 21 / 23]  [Loss: 1.449830, Acc: 29.90%]\n",
      "Accuracy 29.900568181818183\n",
      "Comb\n",
      " [[  0  20   3   0]\n",
      " [  0 123  20   0]\n",
      " [  0 507 294   3]\n",
      " [  0 255 179   4]]\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.00      0.00      1.00      0.00      0.00      0.00        23\n",
      "          1       0.14      0.86      0.38      0.23      0.57      0.34       143\n",
      "          2       0.59      0.37      0.67      0.45      0.49      0.24       804\n",
      "          3       0.57      0.01      1.00      0.02      0.10      0.01       438\n",
      "\n",
      "avg / total       0.53      0.30      0.75      0.29      0.37      0.17      1408\n",
      "\n",
      "Training Epoch 8\n",
      "[Epoch 8/15] [Batch 168 / 170] [Time 1.38 1.58] [Data 0.13 0.18] [Loss: 1.203992, Acc: 54.01%]\n",
      "Epoch time 4.452792096138 mins\n",
      "Testing\n",
      "[Batch 21 / 23]  [Loss: 1.308321, Acc: 42.40%]\n",
      "Accuracy 42.40056818181818\n",
      "Comb\n",
      " [[  2  13   2   6]\n",
      " [ 12  50  39  42]\n",
      " [ 21 139 292 352]\n",
      " [ 13  46 126 253]]\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.04      0.09      0.97      0.06      0.29      0.08        23\n",
      "          1       0.20      0.35      0.84      0.26      0.54      0.28       143\n",
      "          2       0.64      0.36      0.72      0.46      0.51      0.25       804\n",
      "          3       0.39      0.58      0.59      0.46      0.58      0.34       438\n",
      "\n",
      "avg / total       0.50      0.42      0.70      0.44      0.53      0.28      1408\n",
      "\n",
      "Training Epoch 9\n",
      "[Epoch 9/15] [Batch 66 / 170] [Time 1.53 1.67] [Data 0.14 0.25] [Loss: 1.146970, Acc: 56.27%]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "model = Model().to('cuda')\n",
    "#model.load_state_dict(torch.load('./results/face-wacv-100-comp-convlstm+att/save.pth')['state_dict'])\n",
    "model = nn.DataParallel(model)\n",
    "class LogCoshLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "class XTanhLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(ey_t * torch.tanh(ey_t))\n",
    "\n",
    "class XSigmoidLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(2 * ey_t / (1 + torch.exp(-ey_t)) - ey_t)\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from apex import amp\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data,batch_size = batch_size , num_workers = 8,shuffle = True)\n",
    "val_loader = DataLoader(val_data,batch_size = batch_size, num_workers = 8)\n",
    "#test_loader = DataLoader(test_data,batch_size = batch_size , num_workers = 16)\n",
    "class_weights = torch.from_numpy(np.asarray([2,2,1,1])).type(torch.FloatTensor).cuda()\n",
    "optimizer = RAdam(model.parameters(), lr= lr,weight_decay = 1e-3)\n",
    "#model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights , reduction = 'none').cuda()\n",
    "best_acc = 0\n",
    "beta = 0.9999\n",
    "gamma = 0.5\n",
    "samples_per_cls = [544,1065,2615,2493]\n",
    "loss_type = \"focal\"\n",
    "no_of_classes = 4\n",
    "num_epochs = 15\n",
    "onecyc = OneCycle(len(train_loader)*num_epochs, lr)\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    train_epoch(epoch,num_epochs,train_loader,model,criterion,optimizer,train_logger,train_batch_logger,batch_size,onecyc,writer,result_path)\n",
    "    true,pred,best_acc= test(epoch,model,val_loader,criterion,batch_size,result_path,best_acc)\n",
    "    cm1 = confusion_matrix(true,pred)\n",
    "    print('Comb\\n',cm1)\n",
    "    print(classification_report_imbalanced(true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE loss pipeline\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer, epoch_logger, batch_logger, batch_size , onecyc , writer , result_path):\n",
    "    print('Training Epoch {}'.format(epoch))\n",
    "    model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "    start_time = time.time()\n",
    "    end_time = time.time()\n",
    "    t = []\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        data_time.update(time.time() - end_time)\n",
    "        if(inputs.size(0)<batch_size):\n",
    "            continue\n",
    "        if torch.cuda.is_available():\n",
    "            targets = targets.type(torch.cuda.LongTensor)\n",
    "            inputs = inputs.cuda()\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.mean(criterion(outputs,targets.type(torch.cuda.LongTensor)))\n",
    "#        loss2 = criterion((output).type(torch.cuda.FloatTensor), (targets/3).type(torch.cuda.FloatTensor))\n",
    "#        loss = (1-lambda)*loss1 + lambda*loss\n",
    "#        loss = loss2 + loss1\n",
    "#        loss = loss1\n",
    "        acc = calculate_accuracy(outputs, targets.type(torch.cuda.LongTensor))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        accuracies.update(acc, inputs.size(0))\n",
    "        lr,_ = onecyc.calc()\n",
    "        update_lr(optimizer, lr)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_time.update(time.time() - end_time)\n",
    "        end_time = time.time()\n",
    "        batch_logger.log({\n",
    "            'epoch': epoch,\n",
    "            'batch': i + 1,\n",
    "            'iter': (epoch - 1) * len(data_loader) + (i + 1),\n",
    "            'loss': losses.val,\n",
    "            'acc': accuracies.val,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "        writer.add_scalar('data/acc', accuracies.val, (epoch - 1) * len(data_loader) + (i + 1))\n",
    "        writer.add_scalar('data/loss', losses.val, (epoch - 1) * len(data_loader) + (i + 1))  \n",
    "        sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d / %d] [Time %.2f %.2f] [Data %.2f %.2f] [Loss: %f, Acc: %.2f%%]\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    i,\n",
    "                    len(data_loader),\n",
    "                    batch_time.val,\n",
    "                    batch_time.avg,\n",
    "                    data_time.val,\n",
    "                    data_time.avg,\n",
    "                    losses.avg,\n",
    "                    accuracies.avg))\n",
    "    print('\\nEpoch time {} mins'.format((end_time-start_time)/60))\n",
    "    epoch_logger.log({\n",
    "        'epoch': epoch,\n",
    "        'loss': losses.avg,\n",
    "        'acc': accuracies.avg,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "    writer.add_scalar('data/acc_epoch', accuracies.avg, epoch)\n",
    "    writer.add_scalar('data/loss_epoch', losses.avg, epoch)        \n",
    "        \n",
    "    save_file_path = os.path.join(result_path,'save.pth')\n",
    "    states = {\n",
    "        'state_dict': model.state_dict()\n",
    "    }\n",
    "    torch.save(states, save_file_path)\n",
    "    return t\n",
    "def test(epoch,model, data_loader ,criterion, batch_size, result_path,best_acc = 0 , logger = None ):\n",
    "    print('Testing')\n",
    "#    criterion = nn.L1Loss().cuda()\n",
    "    if(epoch == 1):\n",
    "        best_acc = 0\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "    pred = []\n",
    "    true = []\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(data_loader):\n",
    "            if(inputs.size(0)*2<batch_size):\n",
    "                continue\n",
    "            if torch.cuda.is_available():\n",
    "                targets = targets.cuda().type(torch.cuda.FloatTensor)\n",
    "                inputs = inputs.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.mean(criterion(outputs, targets.type(torch.cuda.LongTensor)))\n",
    "            acc = calculate_accuracy(outputs,targets.type(torch.cuda.LongTensor))\n",
    "            _,p = torch.max(outputs,1) \n",
    "            true += (targets.type(torch.cuda.LongTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n",
    "            pred += p.detach().cpu().numpy().reshape(len(p)).tolist()\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            accuracies.update(acc, inputs.size(0))\n",
    "            sys.stdout.write(\n",
    "                    \"\\r[Batch %d / %d]  [Loss: %f, Acc: %.2f%%]\"\n",
    "                    % (\n",
    "                        i,\n",
    "                        len(data_loader),\n",
    "                        losses.avg,\n",
    "                        accuracies.avg\n",
    "                        )\n",
    "                    )\n",
    "        if(logger):\n",
    "            logger.log({'epoch': epoch, 'loss': losses.avg, 'acc': accuracies.avg})\n",
    "        print('\\nAccuracy {}'.format(accuracies.avg))\n",
    "        if(accuracies.avg>best_acc):\n",
    "            best_acc = accuracies.avg\n",
    "            result_path = os.path.join(result_path,'best.pth')\n",
    "            state = {\n",
    "            'acc':best_acc,\n",
    "             'state':model.state_dict()\n",
    "            }\n",
    "            torch.save(state,result_path)\n",
    "    return true,pred,best_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
