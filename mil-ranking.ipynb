{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjaymoto75/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_extract(path):\n",
    "    vidObj = cv2.VideoCapture(path) \n",
    "    success = 1\n",
    "    while success:\n",
    "        success, image = vidObj.read()\n",
    "        if success:\n",
    "            yield image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_label = pd.read_csv('./TrainLabels.csv')\n",
    "valid_label = pd.read_csv('./ValidationLabels.csv')\n",
    "test_label = pd.read_csv('./TestLabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label0 = []\n",
    "clipid0 = []\n",
    "label1 = []\n",
    "clipid1 = []\n",
    "\n",
    "for i in range(len(train_label)):\n",
    "    if(train_label['Engagement'][i]<2):\n",
    "        clipid0.append(train_label['ClipID'][i])\n",
    "    if(train_label['Engagement'][i]>=2):\n",
    "        clipid1.append(train_label['ClipID'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import cv2\n",
    "import sys\n",
    "im_size = 112\n",
    "train_transforms = transforms.Compose([\n",
    "                                        transforms.ToPILImage(),\n",
    "                                        transforms.Resize((im_size,im_size)),\n",
    "                                        transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class video_dataset(Dataset):\n",
    "    def __init__(self,frame_dir,clipid,transform = None):\n",
    "        self.folder = os.listdir(frame_dir)\n",
    "        self.id = clipid\n",
    "        self.frame_dir = frame_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.id)\n",
    "    def __getitem__(self,idx):\n",
    "        id_1 = self.id[idx][:6]\n",
    "        path1 = os.path.join(self.frame_dir,id_1)\n",
    "        id_2 = self.id[idx][:-4]\n",
    "        path2 = os.path.join(path1,id_2)\n",
    "        seq_image = list()\n",
    "        i = 0\n",
    "        path3 = os.path.join(path2,str(i)+'.jpg')\n",
    "        image = cv2.imread(path3)\n",
    "        face_locations = face_recognition.face_locations(image)\n",
    "        flag = 0\n",
    "        if(len(face_locations) == 0):\n",
    "            flag = 1\n",
    "        else:\n",
    "            top,right,bottom,left = face_locations[0]\n",
    "            top = int(top*0.7)\n",
    "            left = int(left*0.6)\n",
    "            right = int(right*1.3)\n",
    "        all_images = []\n",
    "        count = 0\n",
    "        while i<300:\n",
    "            count = count+1\n",
    "            path3 = os.path.join(path2,str(i)+'.jpg')\n",
    "            image = cv2.imread(path3)\n",
    "            if(flag==1):\n",
    "                face_locations = face_recognition.face_locations(image)\n",
    "                if(len(face_locations)>0):\n",
    "                    top,right,bottom,left = face_locations[0]\n",
    "                    flag = 0\n",
    "                    top = int(top*0.7)\n",
    "                    left = int(left*0.6)\n",
    "                    right = int(right*1.3)\n",
    "                    image = image[top:,left:right,:]\n",
    "                    if(self.transform):\n",
    "                        image = self.transform(image)\n",
    "                    seq_image.append(image)\n",
    "                    if(len(seq_image)==16):\n",
    "                        all_images.append(torch.stack(seq_image))\n",
    "                        seq_image = []\n",
    "                    i = i+2\n",
    "                    continue\n",
    "                if(self.transform):\n",
    "                    image = self.transform(image)\n",
    "                seq_image.append(image)\n",
    "                if(len(seq_image)==16):\n",
    "                    all_images.append(torch.stack(seq_image))\n",
    "                    seq_image = []\n",
    "                i = i+2\n",
    "                continue\n",
    "            image = image[top:,left:right,:]\n",
    "            if(self.transform):\n",
    "                image = self.transform(image)\n",
    "            seq_image.append(image)\n",
    "            if(len(seq_image)==16):\n",
    "                all_images.append(torch.stack(seq_image))\n",
    "                seq_image = []\n",
    "            i = i+2\n",
    "        all_images = torch.stack(all_images)\n",
    "        all_images = all_images.permute((0,2,1,3,4))\n",
    "        if(len(all_images)<9):\n",
    "            print(id_1,idx)\n",
    "        return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.5,0.5,0.5]\n",
    "std = [0.5,0.5,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                                        transforms.ToPILImage(),\n",
    "                                        transforms.Resize((im_size,im_size)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean,std)])\n",
    "test_transforms = transforms.Compose([\n",
    "                                        transforms.ToPILImage(),\n",
    "                                        transforms.Resize((im_size,im_size)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean,std)])\n",
    "train_data_0 = video_dataset('./frames/train',clipid0,transform = train_transforms)\n",
    "train_data_1 = video_dataset('./frames/train',clipid1,transform = train_transforms)\n",
    "#val_data = video_dataset('./frames/val',valid_label,sequence_length = 16,transform = test_transforms)\n",
    "#train_loader = DataLoader(train_data,batch_size = 2,num_workers = 4 ,shuffle = True)\n",
    "#valid_loader = DataLoader(val_data,batch_size = 2,num_workers = 4 ,shuffle = True)\n",
    "#dataloaders = {'train':train_loader , 'val':valid_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "#image,_ = train_data[5000]\n",
    "def im_plot(tensor):\n",
    "    image = tensor.cpu().numpy().transpose(1,2,0)\n",
    "    b,g,r = cv2.split(image)\n",
    "    image = cv2.merge((r,g,b))\n",
    "    image = image*[0.5,0.5,0.5] + [0.5,0.5,0.5]\n",
    "    image = image*255.0\n",
    "    plt.imshow(image.astype(int))\n",
    "    plt.show()\n",
    "#im_plot(image[:,10,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sanjaymoto75/attention/representationflowcvpr19\n"
     ]
    }
   ],
   "source": [
    "%cd ./representationflowcvpr19\n",
    "import math\n",
    "\n",
    "import rep_flow_layer as rf\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class SamePadding(nn.Module):\n",
    "\n",
    "  def __init__(self, kernel_size, stride):\n",
    "    super(SamePadding, self).__init__()\n",
    "    self.kernel_size = kernel_size\n",
    "    self.stride = stride\n",
    "\n",
    "  def compute_pad(self, dim, s):\n",
    "    if s % self.stride[dim] == 0:\n",
    "      return max(self.kernel_size[dim] - self.stride[dim], 0)\n",
    "    else:\n",
    "      return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # compute 'same' padding\n",
    "    (batch, channel, t, h, w) = x.size()\n",
    "    #print t,h,w\n",
    "    out_t = np.ceil(float(t) / float(self.stride[0]))\n",
    "    out_h = np.ceil(float(h) / float(self.stride[1]))\n",
    "    out_w = np.ceil(float(w) / float(self.stride[2]))\n",
    "    #print out_t, out_h, out_w\n",
    "    pad_t = self.compute_pad(0, t)\n",
    "    pad_h = self.compute_pad(1, h)\n",
    "    pad_w = self.compute_pad(2, w)\n",
    "    #print pad_t, pad_h, pad_w\n",
    "    \n",
    "    pad_t_f = pad_t // 2\n",
    "    pad_t_b = pad_t - pad_t_f\n",
    "    pad_h_f = pad_h // 2\n",
    "    pad_h_b = pad_h - pad_h_f\n",
    "    pad_w_f = pad_w // 2\n",
    "    pad_w_b = pad_w - pad_w_f\n",
    "    \n",
    "    pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
    "    #print x.size()\n",
    "    #print pad\n",
    "    x = F.pad(x, pad)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Bottleneck3D(nn.Module):\n",
    "  \n",
    "  def __init__(self, inputs, filters, is_training, strides,\n",
    "               use_projection=False, T=3, data_format='channels_last', non_local=False):\n",
    "    \"\"\"Bottleneck block variant for residual networks with BN after convolutions.\n",
    "  Args:\n",
    "    inputs: `Tensor` of size `[batch, channels, height, width]`.\n",
    "    filters: `int` number of filters for the first two convolutions. Note that\n",
    "        the third and final convolution will use 4 times as many filters.\n",
    "    is_training: `bool` for whether the model is in training.\n",
    "    strides: `int` block stride. If greater than 1, this block will ultimately\n",
    "        downsample the input.\n",
    "    use_projection: `bool` for whether this block should use a projection\n",
    "        shortcut (versus the default identity shortcut). This is usually `True`\n",
    "        for the first block of a block group, which may change the number of\n",
    "        filters and the resolution.\n",
    "    data_format: `str` either \"channels_first\" for `[batch, channels, height,\n",
    "        width]` or \"channels_last for `[batch, height, width, channels]`.\n",
    "  Returns:\n",
    "    The output `Tensor` of the block.\n",
    "  \"\"\"\n",
    "    super(Bottleneck3D, self).__init__()\n",
    "            \n",
    "    df = 'NDHWC' if data_format == 'channels_last' else 'NCDHW'\n",
    "    self.shortcut = None\n",
    "    if use_projection:\n",
    "      # Projection shortcut only in first block within a group. Bottleneck blocks\n",
    "      # end with 4 times the number of filters.\n",
    "      filters_out = 4 * filters\n",
    "      self.shortcut = nn.Sequential(SamePadding((1,1,1),(1,strides,strides)),\n",
    "                                    nn.Conv3d(\n",
    "                                      inputs, filters_out, kernel_size=1, stride=(1,strides,strides), bias=False, padding=0),\n",
    "                                    nn.BatchNorm3d(filters_out),\n",
    "                                    nn.BatchNorm3d(filters_out)) # there are two, due to old models having it. To load weights, 2 batch norms are needed here...\n",
    "    \n",
    "    self.layers = nn.Sequential(SamePadding((T,1,1), (1,1,1)),\n",
    "                                nn.Conv3d(inputs, filters, kernel_size=(T,1,1), stride=1, padding=(0,0,0), bias=False), #1\n",
    "                                nn.BatchNorm3d(filters), #2\n",
    "                                nn.ReLU(),\n",
    "                                SamePadding((1,3,3),(1,strides,strides)),\n",
    "                                nn.Conv3d(filters, filters, kernel_size=(1,3,3), stride=(1,strides,strides), bias=False, padding=0), #5\n",
    "                                nn.BatchNorm3d(filters),#6\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv3d(filters, 4*filters, kernel_size=1, stride=1, bias=False, padding=0),#8\n",
    "                                nn.BatchNorm3d(4*filters))#9\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    #print('block', x.size())\n",
    "    if self.shortcut:\n",
    "      res = self.shortcut(x)\n",
    "    else:\n",
    "      res = x\n",
    "    #print('b2',x.size())\n",
    "    return F.relu(self.layers(x) + res)\n",
    "\n",
    "\n",
    "  \n",
    "class Block3D(nn.Module):\n",
    "  def __init__(self, inputs, filters, block_fn, blocks, strides, is_training, name,\n",
    "                   data_format='channels_last', non_local=0):\n",
    "    \"\"\"Creates one group of blocks for the ResNet model.\n",
    "    \n",
    "  Args:\n",
    "    inputs: `Tensor` of size `[batch, channels, height, width]`.\n",
    "    filters: `int` number of filters for the first convolution of the layer.\n",
    "    block_fn: `function` for the block to use within the model\n",
    "    blocks: `int` number of blocks contained in the layer.\n",
    "    strides: `int` stride to use for the first convolution of the layer. If\n",
    "        greater than 1, this layer will downsample the input.\n",
    "    is_training: `bool` for whether the model is training.\n",
    "    name: `str`name for the Tensor output of the block layer.\n",
    "    data_format: `str` either \"channels_first\" for `[batch, channels, height,\n",
    "        width]` or \"channels_last for `[batch, height, width, channels]`.\n",
    "  Returns:\n",
    "    The output `Tensor` of the block layer.\n",
    "    \"\"\"\n",
    "    super(Block3D, self).__init__()\n",
    "\n",
    "    self.blocks = nn.ModuleList()\n",
    "    # Only the first block per block_group uses projection shortcut and strides.\n",
    "    self.blocks.append(Bottleneck3D(inputs, filters, is_training, strides,\n",
    "                                    use_projection=True, data_format=data_format))\n",
    "    inputs = filters * 4\n",
    "    T = 3\n",
    "    for i in range(1, blocks):\n",
    "      self.blocks.append(Bottleneck3D(inputs, filters, is_training, 1, T=T,\n",
    "                                      data_format=data_format, non_local=0))\n",
    "      # only use 1 3D conv per 2 residual blocks (per Non-local NN paper)\n",
    "      T = (3 if T==1 else 1)\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    for block in self.blocks:\n",
    "      x = block(x)\n",
    "    return x\n",
    "\n",
    "class ResNet3D(nn.Module):\n",
    "  \n",
    "  def __init__(self, block_fn, layers, num_classes,\n",
    "               data_format='channels_last', non_local=[], rep_flow=[],\n",
    "               dropout_keep_prob=0.5):\n",
    "    \"\"\"Generator for ResNet v1 models.\n",
    "  Args:\n",
    "    block_fn: `function` for the block to use within the model. Either\n",
    "        `residual_block` or `bottleneck_block`.\n",
    "    layers: list of 4 `int`s denoting the number of blocks to include in each\n",
    "      of the 4 block groups. Each group consists of blocks that take inputs of\n",
    "      the same resolution.\n",
    "    num_classes: `int` number of possible classes for image classification.\n",
    "    data_format: `str` either \"channels_first\" for `[batch, channels, height,\n",
    "        width]` or \"channels_last for `[batch, height, width, channels]`.\n",
    "  Returns:\n",
    "    Model `function` that takes in `inputs` and `is_training` and returns the\n",
    "    output `Tensor` of the ResNet model.\n",
    "    \"\"\"\n",
    "    super(ResNet3D, self).__init__()\n",
    "    is_training = False # no effect in pytorch\n",
    "\n",
    "    # TODO: RF Layer!\n",
    "    #self.rep_flow = rep_flow_layer.rep_flow(inputs, rep_flow[0], is_training, bottleneck=1, data_format=data_format, use_last_conv=False)\n",
    "\n",
    "    \"\"\"Creation of the model graph.\"\"\"\n",
    "    self.stem = nn.Conv3d(\n",
    "      3, 64, kernel_size=7, bias=False, stride=2)\n",
    "    \n",
    "    self.bn1 = nn.BatchNorm3d(64, eps=0.001, momentum=0.01)\n",
    "    self.relu = nn.ReLU(inplace=True)\n",
    "    self.pad = SamePadding((3,3,3),(2,2,2))\n",
    "    self.maxpool = nn.MaxPool3d(kernel_size=3,\n",
    "                                stride=2, padding=0)\n",
    "\n",
    "    self.rep_flow = rf.FlowLayer(512)\n",
    "\n",
    "    # res 2\n",
    "    inputs = 64\n",
    "    self.res2 = Block3D(\n",
    "      inputs=inputs, filters=64, block_fn=block_fn, blocks=layers[0],\n",
    "      strides=1, is_training=is_training, name='block_group1',\n",
    "      data_format=data_format, non_local=non_local[0])\n",
    "    \n",
    "    # res 3\n",
    "    inputs = 64*4\n",
    "    self.res3 = Block3D(\n",
    "      inputs=inputs, filters=128, block_fn=block_fn, blocks=layers[1],\n",
    "      strides=2, is_training=is_training, name='block_group2',\n",
    "      data_format=data_format, non_local=non_local[1])\n",
    "\n",
    "    # res 4\n",
    "    inputs = 128*4\n",
    "    self.res4 = Block3D(\n",
    "      inputs=inputs, filters=256, block_fn=block_fn, blocks=layers[2],\n",
    "      strides=2, is_training=is_training, name='block_group3',\n",
    "      data_format=data_format, non_local=non_local[2])\n",
    "\n",
    "    # res 5\n",
    "    inputs = 256*4\n",
    "    self.res5 = Block3D(\n",
    "        inputs=inputs, filters=512, block_fn=block_fn, blocks=layers[3],\n",
    "        strides=2, is_training=is_training, name='block_group4',\n",
    "        data_format=data_format, non_local=non_local[3])\n",
    "\n",
    "    self.dropout = nn.Dropout(0.5)\n",
    "    self.classify = nn.Conv3d(512*4, num_classes, kernel_size=1, stride=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.stem(x)\n",
    "    x = self.bn1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.maxpool(self.pad(x))\n",
    "    x = self.res2(x)\n",
    "    x = self.res3(x)\n",
    "\n",
    "    x = self.rep_flow(x)\n",
    "\n",
    "    x = self.res4(x)\n",
    "    x = self.res5(x)\n",
    "    x = x.mean(3).mean(3).unsqueeze(3).unsqueeze(3) # spatial average\n",
    "    x = self.dropout(x)\n",
    "    x = self.classify(x)\n",
    "    x = x.mean(2) # temporal average\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_3d_v1(resnet_depth, num_classes, data_format='channels_last', is_3d=True, non_local=[0,0,0,0], rep_flow=[0,0,0,0,0]):\n",
    "  \"\"\"Returns the ResNet model for a given size and number of output classes.\"\"\"\n",
    "  model_params = {\n",
    "      18: {'block': None, 'layers': [2, 2, 2, 2]},\n",
    "      34: {'block': None, 'layers': [3, 4, 6, 3]},\n",
    "      50: {'block': None, 'layers': [3, 4, 6, 3]},\n",
    "      101: {'block': None, 'layers': [3, 4, 23, 3]},\n",
    "      152: {'block': None, 'layers': [3, 8, 36, 3]},\n",
    "      200: {'block': None, 'layers': [3, 24, 36, 3]}\n",
    "  }\n",
    "\n",
    "  if resnet_depth not in model_params:\n",
    "    raise ValueError('Not a valid resnet_depth:', resnet_depth)\n",
    "\n",
    "  params = model_params[resnet_depth]\n",
    "  return ResNet3D(\n",
    "    params['block'], params['layers'], num_classes, data_format, non_local, rep_flow)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net = resnet_3d_v1(50, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sanjaymoto75/attention\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1        [-1, 64, 5, 53, 53]          65,856\n",
      "       BatchNorm3d-2        [-1, 64, 5, 53, 53]             128\n",
      "       SamePadding-3        [-1, 64, 7, 55, 55]               0\n",
      "         MaxPool3d-4        [-1, 64, 3, 27, 27]               0\n",
      "       SamePadding-5        [-1, 64, 3, 27, 27]               0\n",
      "            Conv3d-6       [-1, 256, 3, 27, 27]          16,384\n",
      "       BatchNorm3d-7       [-1, 256, 3, 27, 27]             512\n",
      "       BatchNorm3d-8       [-1, 256, 3, 27, 27]             512\n",
      "       SamePadding-9        [-1, 64, 5, 27, 27]               0\n",
      "           Conv3d-10        [-1, 64, 3, 27, 27]          12,288\n",
      "      BatchNorm3d-11        [-1, 64, 3, 27, 27]             128\n",
      "             ReLU-12        [-1, 64, 3, 27, 27]               0\n",
      "      SamePadding-13        [-1, 64, 3, 29, 29]               0\n",
      "           Conv3d-14        [-1, 64, 3, 27, 27]          36,864\n",
      "      BatchNorm3d-15        [-1, 64, 3, 27, 27]             128\n",
      "             ReLU-16        [-1, 64, 3, 27, 27]               0\n",
      "           Conv3d-17       [-1, 256, 3, 27, 27]          16,384\n",
      "      BatchNorm3d-18       [-1, 256, 3, 27, 27]             512\n",
      "     Bottleneck3D-19       [-1, 256, 3, 27, 27]               0\n",
      "      SamePadding-20       [-1, 256, 5, 27, 27]               0\n",
      "           Conv3d-21        [-1, 64, 3, 27, 27]          49,152\n",
      "      BatchNorm3d-22        [-1, 64, 3, 27, 27]             128\n",
      "             ReLU-23        [-1, 64, 3, 27, 27]               0\n",
      "      SamePadding-24        [-1, 64, 3, 29, 29]               0\n",
      "           Conv3d-25        [-1, 64, 3, 27, 27]          36,864\n",
      "      BatchNorm3d-26        [-1, 64, 3, 27, 27]             128\n",
      "             ReLU-27        [-1, 64, 3, 27, 27]               0\n",
      "           Conv3d-28       [-1, 256, 3, 27, 27]          16,384\n",
      "      BatchNorm3d-29       [-1, 256, 3, 27, 27]             512\n",
      "     Bottleneck3D-30       [-1, 256, 3, 27, 27]               0\n",
      "      SamePadding-31       [-1, 256, 3, 27, 27]               0\n",
      "           Conv3d-32        [-1, 64, 3, 27, 27]          16,384\n",
      "      BatchNorm3d-33        [-1, 64, 3, 27, 27]             128\n",
      "             ReLU-34        [-1, 64, 3, 27, 27]               0\n",
      "      SamePadding-35        [-1, 64, 3, 29, 29]               0\n",
      "           Conv3d-36        [-1, 64, 3, 27, 27]          36,864\n",
      "      BatchNorm3d-37        [-1, 64, 3, 27, 27]             128\n",
      "             ReLU-38        [-1, 64, 3, 27, 27]               0\n",
      "           Conv3d-39       [-1, 256, 3, 27, 27]          16,384\n",
      "      BatchNorm3d-40       [-1, 256, 3, 27, 27]             512\n",
      "     Bottleneck3D-41       [-1, 256, 3, 27, 27]               0\n",
      "          Block3D-42       [-1, 256, 3, 27, 27]               0\n",
      "      SamePadding-43       [-1, 256, 3, 27, 27]               0\n",
      "           Conv3d-44       [-1, 512, 3, 14, 14]         131,072\n",
      "      BatchNorm3d-45       [-1, 512, 3, 14, 14]           1,024\n",
      "      BatchNorm3d-46       [-1, 512, 3, 14, 14]           1,024\n",
      "      SamePadding-47       [-1, 256, 5, 27, 27]               0\n",
      "           Conv3d-48       [-1, 128, 3, 27, 27]          98,304\n",
      "      BatchNorm3d-49       [-1, 128, 3, 27, 27]             256\n",
      "             ReLU-50       [-1, 128, 3, 27, 27]               0\n",
      "      SamePadding-51       [-1, 128, 3, 29, 29]               0\n",
      "           Conv3d-52       [-1, 128, 3, 14, 14]         147,456\n",
      "      BatchNorm3d-53       [-1, 128, 3, 14, 14]             256\n",
      "             ReLU-54       [-1, 128, 3, 14, 14]               0\n",
      "           Conv3d-55       [-1, 512, 3, 14, 14]          65,536\n",
      "      BatchNorm3d-56       [-1, 512, 3, 14, 14]           1,024\n",
      "     Bottleneck3D-57       [-1, 512, 3, 14, 14]               0\n",
      "      SamePadding-58       [-1, 512, 5, 14, 14]               0\n",
      "           Conv3d-59       [-1, 128, 3, 14, 14]         196,608\n",
      "      BatchNorm3d-60       [-1, 128, 3, 14, 14]             256\n",
      "             ReLU-61       [-1, 128, 3, 14, 14]               0\n",
      "      SamePadding-62       [-1, 128, 3, 16, 16]               0\n",
      "           Conv3d-63       [-1, 128, 3, 14, 14]         147,456\n",
      "      BatchNorm3d-64       [-1, 128, 3, 14, 14]             256\n",
      "             ReLU-65       [-1, 128, 3, 14, 14]               0\n",
      "           Conv3d-66       [-1, 512, 3, 14, 14]          65,536\n",
      "      BatchNorm3d-67       [-1, 512, 3, 14, 14]           1,024\n",
      "     Bottleneck3D-68       [-1, 512, 3, 14, 14]               0\n",
      "      SamePadding-69       [-1, 512, 3, 14, 14]               0\n",
      "           Conv3d-70       [-1, 128, 3, 14, 14]          65,536\n",
      "      BatchNorm3d-71       [-1, 128, 3, 14, 14]             256\n",
      "             ReLU-72       [-1, 128, 3, 14, 14]               0\n",
      "      SamePadding-73       [-1, 128, 3, 16, 16]               0\n",
      "           Conv3d-74       [-1, 128, 3, 14, 14]         147,456\n",
      "      BatchNorm3d-75       [-1, 128, 3, 14, 14]             256\n",
      "             ReLU-76       [-1, 128, 3, 14, 14]               0\n",
      "           Conv3d-77       [-1, 512, 3, 14, 14]          65,536\n",
      "      BatchNorm3d-78       [-1, 512, 3, 14, 14]           1,024\n",
      "     Bottleneck3D-79       [-1, 512, 3, 14, 14]               0\n",
      "      SamePadding-80       [-1, 512, 5, 14, 14]               0\n",
      "           Conv3d-81       [-1, 128, 3, 14, 14]         196,608\n",
      "      BatchNorm3d-82       [-1, 128, 3, 14, 14]             256\n",
      "             ReLU-83       [-1, 128, 3, 14, 14]               0\n",
      "      SamePadding-84       [-1, 128, 3, 16, 16]               0\n",
      "           Conv3d-85       [-1, 128, 3, 14, 14]         147,456\n",
      "      BatchNorm3d-86       [-1, 128, 3, 14, 14]             256\n",
      "             ReLU-87       [-1, 128, 3, 14, 14]               0\n",
      "           Conv3d-88       [-1, 512, 3, 14, 14]          65,536\n",
      "      BatchNorm3d-89       [-1, 512, 3, 14, 14]           1,024\n",
      "     Bottleneck3D-90       [-1, 512, 3, 14, 14]               0\n",
      "          Block3D-91       [-1, 512, 3, 14, 14]               0\n",
      "           Conv3d-92        [-1, 32, 3, 14, 14]          16,384\n",
      "           Conv3d-93       [-1, 512, 2, 14, 14]          32,768\n",
      "      BatchNorm3d-94       [-1, 512, 2, 14, 14]           1,024\n",
      "        FlowLayer-95       [-1, 512, 2, 14, 14]               0\n",
      "      SamePadding-96       [-1, 512, 2, 14, 14]               0\n",
      "           Conv3d-97        [-1, 1024, 2, 7, 7]         524,288\n",
      "      BatchNorm3d-98        [-1, 1024, 2, 7, 7]           2,048\n",
      "      BatchNorm3d-99        [-1, 1024, 2, 7, 7]           2,048\n",
      "     SamePadding-100       [-1, 512, 4, 14, 14]               0\n",
      "          Conv3d-101       [-1, 256, 2, 14, 14]         393,216\n",
      "     BatchNorm3d-102       [-1, 256, 2, 14, 14]             512\n",
      "            ReLU-103       [-1, 256, 2, 14, 14]               0\n",
      "     SamePadding-104       [-1, 256, 2, 15, 15]               0\n",
      "          Conv3d-105         [-1, 256, 2, 7, 7]         589,824\n",
      "     BatchNorm3d-106         [-1, 256, 2, 7, 7]             512\n",
      "            ReLU-107         [-1, 256, 2, 7, 7]               0\n",
      "          Conv3d-108        [-1, 1024, 2, 7, 7]         262,144\n",
      "     BatchNorm3d-109        [-1, 1024, 2, 7, 7]           2,048\n",
      "    Bottleneck3D-110        [-1, 1024, 2, 7, 7]               0\n",
      "     SamePadding-111        [-1, 1024, 4, 7, 7]               0\n",
      "          Conv3d-112         [-1, 256, 2, 7, 7]         786,432\n",
      "     BatchNorm3d-113         [-1, 256, 2, 7, 7]             512\n",
      "            ReLU-114         [-1, 256, 2, 7, 7]               0\n",
      "     SamePadding-115         [-1, 256, 2, 9, 9]               0\n",
      "          Conv3d-116         [-1, 256, 2, 7, 7]         589,824\n",
      "     BatchNorm3d-117         [-1, 256, 2, 7, 7]             512\n",
      "            ReLU-118         [-1, 256, 2, 7, 7]               0\n",
      "          Conv3d-119        [-1, 1024, 2, 7, 7]         262,144\n",
      "     BatchNorm3d-120        [-1, 1024, 2, 7, 7]           2,048\n",
      "    Bottleneck3D-121        [-1, 1024, 2, 7, 7]               0\n",
      "     SamePadding-122        [-1, 1024, 2, 7, 7]               0\n",
      "          Conv3d-123         [-1, 256, 2, 7, 7]         262,144\n",
      "     BatchNorm3d-124         [-1, 256, 2, 7, 7]             512\n",
      "            ReLU-125         [-1, 256, 2, 7, 7]               0\n",
      "     SamePadding-126         [-1, 256, 2, 9, 9]               0\n",
      "          Conv3d-127         [-1, 256, 2, 7, 7]         589,824\n",
      "     BatchNorm3d-128         [-1, 256, 2, 7, 7]             512\n",
      "            ReLU-129         [-1, 256, 2, 7, 7]               0\n",
      "          Conv3d-130        [-1, 1024, 2, 7, 7]         262,144\n",
      "     BatchNorm3d-131        [-1, 1024, 2, 7, 7]           2,048\n",
      "    Bottleneck3D-132        [-1, 1024, 2, 7, 7]               0\n",
      "     SamePadding-133        [-1, 1024, 4, 7, 7]               0\n",
      "          Conv3d-134         [-1, 256, 2, 7, 7]         786,432\n",
      "     BatchNorm3d-135         [-1, 256, 2, 7, 7]             512\n",
      "            ReLU-136         [-1, 256, 2, 7, 7]               0\n",
      "     SamePadding-137         [-1, 256, 2, 9, 9]               0\n",
      "          Conv3d-138         [-1, 256, 2, 7, 7]         589,824\n",
      "     BatchNorm3d-139         [-1, 256, 2, 7, 7]             512\n",
      "            ReLU-140         [-1, 256, 2, 7, 7]               0\n",
      "          Conv3d-141        [-1, 1024, 2, 7, 7]         262,144\n",
      "     BatchNorm3d-142        [-1, 1024, 2, 7, 7]           2,048\n",
      "    Bottleneck3D-143        [-1, 1024, 2, 7, 7]               0\n",
      "     SamePadding-144        [-1, 1024, 2, 7, 7]               0\n",
      "          Conv3d-145         [-1, 256, 2, 7, 7]         262,144\n",
      "     BatchNorm3d-146         [-1, 256, 2, 7, 7]             512\n",
      "            ReLU-147         [-1, 256, 2, 7, 7]               0\n",
      "     SamePadding-148         [-1, 256, 2, 9, 9]               0\n",
      "          Conv3d-149         [-1, 256, 2, 7, 7]         589,824\n",
      "     BatchNorm3d-150         [-1, 256, 2, 7, 7]             512\n",
      "            ReLU-151         [-1, 256, 2, 7, 7]               0\n",
      "          Conv3d-152        [-1, 1024, 2, 7, 7]         262,144\n",
      "     BatchNorm3d-153        [-1, 1024, 2, 7, 7]           2,048\n",
      "    Bottleneck3D-154        [-1, 1024, 2, 7, 7]               0\n",
      "     SamePadding-155        [-1, 1024, 4, 7, 7]               0\n",
      "          Conv3d-156         [-1, 256, 2, 7, 7]         786,432\n",
      "     BatchNorm3d-157         [-1, 256, 2, 7, 7]             512\n",
      "            ReLU-158         [-1, 256, 2, 7, 7]               0\n",
      "     SamePadding-159         [-1, 256, 2, 9, 9]               0\n",
      "          Conv3d-160         [-1, 256, 2, 7, 7]         589,824\n",
      "     BatchNorm3d-161         [-1, 256, 2, 7, 7]             512\n",
      "            ReLU-162         [-1, 256, 2, 7, 7]               0\n",
      "          Conv3d-163        [-1, 1024, 2, 7, 7]         262,144\n",
      "     BatchNorm3d-164        [-1, 1024, 2, 7, 7]           2,048\n",
      "    Bottleneck3D-165        [-1, 1024, 2, 7, 7]               0\n",
      "         Block3D-166        [-1, 1024, 2, 7, 7]               0\n",
      "     SamePadding-167        [-1, 1024, 2, 7, 7]               0\n",
      "          Conv3d-168        [-1, 2048, 2, 4, 4]       2,097,152\n",
      "     BatchNorm3d-169        [-1, 2048, 2, 4, 4]           4,096\n",
      "     BatchNorm3d-170        [-1, 2048, 2, 4, 4]           4,096\n",
      "     SamePadding-171        [-1, 1024, 4, 7, 7]               0\n",
      "          Conv3d-172         [-1, 512, 2, 7, 7]       1,572,864\n",
      "     BatchNorm3d-173         [-1, 512, 2, 7, 7]           1,024\n",
      "            ReLU-174         [-1, 512, 2, 7, 7]               0\n",
      "     SamePadding-175         [-1, 512, 2, 9, 9]               0\n",
      "          Conv3d-176         [-1, 512, 2, 4, 4]       2,359,296\n",
      "     BatchNorm3d-177         [-1, 512, 2, 4, 4]           1,024\n",
      "            ReLU-178         [-1, 512, 2, 4, 4]               0\n",
      "          Conv3d-179        [-1, 2048, 2, 4, 4]       1,048,576\n",
      "     BatchNorm3d-180        [-1, 2048, 2, 4, 4]           4,096\n",
      "    Bottleneck3D-181        [-1, 2048, 2, 4, 4]               0\n",
      "     SamePadding-182        [-1, 2048, 4, 4, 4]               0\n",
      "          Conv3d-183         [-1, 512, 2, 4, 4]       3,145,728\n",
      "     BatchNorm3d-184         [-1, 512, 2, 4, 4]           1,024\n",
      "            ReLU-185         [-1, 512, 2, 4, 4]               0\n",
      "     SamePadding-186         [-1, 512, 2, 6, 6]               0\n",
      "          Conv3d-187         [-1, 512, 2, 4, 4]       2,359,296\n",
      "     BatchNorm3d-188         [-1, 512, 2, 4, 4]           1,024\n",
      "            ReLU-189         [-1, 512, 2, 4, 4]               0\n",
      "          Conv3d-190        [-1, 2048, 2, 4, 4]       1,048,576\n",
      "     BatchNorm3d-191        [-1, 2048, 2, 4, 4]           4,096\n",
      "    Bottleneck3D-192        [-1, 2048, 2, 4, 4]               0\n",
      "     SamePadding-193        [-1, 2048, 2, 4, 4]               0\n",
      "          Conv3d-194         [-1, 512, 2, 4, 4]       1,048,576\n",
      "     BatchNorm3d-195         [-1, 512, 2, 4, 4]           1,024\n",
      "            ReLU-196         [-1, 512, 2, 4, 4]               0\n",
      "     SamePadding-197         [-1, 512, 2, 6, 6]               0\n",
      "          Conv3d-198         [-1, 512, 2, 4, 4]       2,359,296\n",
      "     BatchNorm3d-199         [-1, 512, 2, 4, 4]           1,024\n",
      "            ReLU-200         [-1, 512, 2, 4, 4]               0\n",
      "          Conv3d-201        [-1, 2048, 2, 4, 4]       1,048,576\n",
      "     BatchNorm3d-202        [-1, 2048, 2, 4, 4]           4,096\n",
      "    Bottleneck3D-203        [-1, 2048, 2, 4, 4]               0\n",
      "         Block3D-204        [-1, 2048, 2, 4, 4]               0\n",
      "         Dropout-205        [-1, 2048, 2, 1, 1]               0\n",
      "          Conv3d-206           [-1, 1, 2, 1, 1]           2,049\n",
      "         Sigmoid-207           [-1, 1, 2, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 28,973,761\n",
      "Trainable params: 28,973,761\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 257.00\n",
      "Params size (MB): 110.53\n",
      "Estimated Total Size (MB): 369.83\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%cd /home/sanjaymoto75/attention/\n",
    "from torchsummary import summary\n",
    "net = resnet_3d_v1(50, 400).to('cuda')\n",
    "net.load_state_dict(torch.load(\"./data.pt\"))\n",
    "net.classify = nn.Sequential(nn.Conv3d(512*4, 1, kernel_size=1, stride=1),nn.Sigmoid()).to('cuda')\n",
    "summary(net,(3,16,112,112))\n",
    "net.load_state_dict(torch.load('nn_crime.pt'))\n",
    "model = nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(optimizer, lr):\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "def update_mom(optimizer, mom):\n",
    "    for g in optimizer.param_groups:\n",
    "        g['momentum'] = mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modular_code.lr_finder import *\n",
    "import adabound\n",
    "from RAdam.radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_finder(model,train_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_ft = RAdam(model.parameters(), lr=1e-6)\n",
    "    lr_finder = LRFinder(model, optimizer_ft, criterion, device=device)\n",
    "    lr_finder.range_test(train_loader, end_lr=1, num_iter=1000)\n",
    "    lr_finder.reset()\n",
    "    lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mil_loss(anom_pred, normal_pred, k1 = 8e-5, k2 = 8e-5):\n",
    "    batch_size = len(anom_pred)\n",
    "    loss = []\n",
    "    hl1 = []\n",
    "    for j in range(batch_size):\n",
    "        m1 = torch.max(anom_pred[j])\n",
    "        m2 = torch.max(normal_pred[j])\n",
    "        hl = 0\n",
    "        for i in range(len(normal_pred[j])):\n",
    "            m2 = max(0,1-m1+normal_pred[j][i])\n",
    "            m3 = max(0,normal_pred[j][i]-torch.log2(m1))\n",
    "            hl = hl + m2+m3\n",
    "        hl = hl/len(normal_pred)\n",
    "        h = max(0,1-m1+m2)\n",
    "        hl1.append(h)\n",
    "        smooth_loss = 0\n",
    "        for i in range(len(anom_pred[j])-1):\n",
    "            smooth_loss = smooth_loss + torch.pow(anom_pred[j][i]-anom_pred[j][i+1], 2) \n",
    "        smooth_loss = smooth_loss*k1\n",
    "        sparse_loss = k2 * torch.sum(anom_pred[j])\n",
    "        loss1 = hl + smooth_loss + sparse_loss\n",
    "        loss.append(loss1)\n",
    "    return torch.sum(torch.stack(loss).type(torch.FloatTensor)),torch.mean(torch.stack(hl1).type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 0 ---\n",
      "\n",
      "--- Phase train ---\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "import face_recognition\n",
    "batch_size = 10\n",
    "optimizer = RAdam(model.parameters(), lr=0.01)\n",
    "dl0 = DataLoader(train_data_0,batch_size = batch_size,shuffle=  True,num_workers = 4)\n",
    "dl1 = DataLoader(train_data_1,batch_size = batch_size,shuffle=  True,num_workers = 4)\n",
    "for epoch in range(num_epochs):\n",
    "    print('')\n",
    "    print(\"--- Epoch {} ---\".format(epoch))\n",
    "    phase1 = ['train']\n",
    "    for phase in phase1:\n",
    "        print('')\n",
    "        print(\"--- Phase {} ---\".format(phase))\n",
    "        epoch_metrics = {\"loss\": [], \"hl\":[]}\n",
    "        batch_i = 0\n",
    "        if(phase == 'train'):\n",
    "            model = model.train()\n",
    "        else:\n",
    "            model = model.eval()\n",
    "        for j in range(300):\n",
    "            optimizer.zero_grad()\n",
    "            batch = []\n",
    "            images_0 = next(iter(dl0))\n",
    "            images_0 = torch.reshape(images_0,(-1,3,16,112,112))\n",
    "            predictions_0 = model(images_0.type(torch.cuda.FloatTensor))\n",
    "            del images_0\n",
    "            images_1 = next(iter(dl1))\n",
    "            images_1 = torch.reshape(images_1,(-1,3,16,112,112))\n",
    "            predictions_1 = model(images_1.type(torch.cuda.FloatTensor))\n",
    "            del images_1\n",
    "            loss,h = mil_loss(predictions_0.squeeze().reshape(batch_size,9),predictions_1.squeeze().reshape(batch_size,9))\n",
    "            loss.cuda().backward()\n",
    "            optimizer.step()\n",
    "            epoch_metrics[\"loss\"].append(loss.item())\n",
    "            try:\n",
    "                epoch_metrics[\"hl\"].append(np.mean(h.detach().cpu().numpy()))\n",
    "            except:\n",
    "                epoch_metrics[\"hl\"].append(0.)\n",
    "            sys.stdout.write(\n",
    "                  \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f)] [Hinge Loss: (%f)]\"\n",
    "                  % (\n",
    "                      epoch,\n",
    "                      num_epochs,\n",
    "                      j,\n",
    "                      300,\n",
    "                      loss.item(),\n",
    "                      np.mean(epoch_metrics[\"loss\"]),\n",
    "                      np.mean(epoch_metrics[\"hl\"])\n",
    "                  )\n",
    "              )\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            batch_i += 1\n",
    "            if(j%100==0):\n",
    "                torch.save(model.module.state_dict(),'./nn_crime.pt')\n",
    "        print('')\n",
    "        print('{} , Loss: {}'.format(phase,np.mean(epoch_metrics[\"loss\"])))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
