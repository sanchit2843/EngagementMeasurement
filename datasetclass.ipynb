{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjaymoto75/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClipID</th>\n",
       "      <th>Boredom</th>\n",
       "      <th>Engagement</th>\n",
       "      <th>Confusion</th>\n",
       "      <th>Frustration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100011002.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100011003.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1100011004.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1100011005.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1100011006.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1100011007.avi</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1100011008.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1100011009.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1100011010.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1100011011.avi</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ClipID  Boredom  Engagement  Confusion  Frustration \n",
       "0  1100011002.avi        0           2          0             0\n",
       "1  1100011003.avi        0           2          0             0\n",
       "2  1100011004.avi        0           3          0             0\n",
       "3  1100011005.avi        0           3          0             0\n",
       "4  1100011006.avi        0           3          0             0\n",
       "5  1100011007.avi        1           2          0             0\n",
       "6  1100011008.avi        0           3          0             0\n",
       "7  1100011009.avi        0           2          1             0\n",
       "8  1100011010.avi        0           3          0             0\n",
       "9  1100011011.avi        0           3          0             0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_label = pd.read_csv('./TrainLabels.csv')\n",
    "train_label.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "im_size = 224\n",
    "train_transforms = transforms.Compose([\n",
    "                                        transforms.ToPILImage(),\n",
    "                                        transforms.Resize((im_size,im_size)),\n",
    "                                        transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class video_dataset(Dataset):\n",
    "    def __init__(self,frame_dir,train_csv,sequencelength = 60,skip_length = 5,transform = None):\n",
    "        self.folder = os.listdir(frame_dir)\n",
    "        self.id = train_csv['ClipID']\n",
    "        self.engagement = train_csv['Engagement']\n",
    "        self.frame_dir = frame_dir\n",
    "        self.transform = transform\n",
    "        self.not_exist = list()\n",
    "    def __len__(self):\n",
    "        return len(self.id)\n",
    "    def __getitem__(self,idx):\n",
    "        id_1 = self.id[idx][:6]\n",
    "        path1 = os.path.join(self.frame_dir,id_1)\n",
    "        id_2 = self.id[idx][:-4]\n",
    "        path2 = os.path.join(path1,id_2)\n",
    "        seq_image = list()\n",
    "        i = 0\n",
    "        while i<300:\n",
    "            path3 = os.path.join(path2,str(i)+'.jpg')\n",
    "            image = cv2.imread(path3)\n",
    "            if(self.transform):\n",
    "                image = self.transform(image)\n",
    "            seq_image.append(image)\n",
    "            i = i+8\n",
    "        seq_image = torch.stack(seq_image)\n",
    "        label = self.engagement[idx]\n",
    "        return seq_image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = video_dataset('./frames/train',train_label,transform = train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import resnet152\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "##############################\n",
    "#         Encoder\n",
    "##############################\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.resnet = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        self.final = nn.Sequential(nn.Linear(1000, latent_dim), nn.BatchNorm1d(latent_dim, momentum=0.01))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.final(x)\n",
    "\n",
    "\n",
    "##############################\n",
    "#           LSTM\n",
    "##############################\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, latent_dim, num_layers, hidden_dim, bidirectional):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, self.hidden_state = self.lstm(x, self.hidden_state)\n",
    "        return x\n",
    "\n",
    "\n",
    "##############################\n",
    "#      Attention Module\n",
    "##############################\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.latent_attention = nn.Linear(latent_dim, attention_dim)\n",
    "        self.hidden_attention = nn.Linear(hidden_dim, attention_dim)\n",
    "        self.joint_attention = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, latent_repr, hidden_repr):\n",
    "        if hidden_repr is None:\n",
    "            hidden_repr = [\n",
    "                Variable(\n",
    "                    torch.zeros(latent_repr.size(0), 1, self.hidden_attention.in_features), requires_grad=False\n",
    "                ).float()\n",
    "            ]\n",
    "        h_t = hidden_repr[0]\n",
    "        latent_att = self.latent_attention(latent_att)\n",
    "        hidden_att = self.hidden_attention(h_t)\n",
    "        joint_att = self.joint_attention(F.relu(latent_att + hidden_att)).squeeze(-1)\n",
    "        attention_w = F.softmax(joint_att, dim=-1)\n",
    "        return attention_w\n",
    "\n",
    "\n",
    "##############################\n",
    "#         ConvLSTM\n",
    "##############################\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_classes, latent_dim=512, lstm_layers=1, hidden_dim=1024, bidirectional=True, attention=True\n",
    "    ):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.lstm = LSTM(latent_dim, lstm_layers, hidden_dim, bidirectional)\n",
    "        self.output_layers = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim if bidirectional else hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        self.attention = attention\n",
    "        self.attention_layer = nn.Linear(2 * hidden_dim if bidirectional else hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(batch_size, seq_length, -1)\n",
    "        x = self.lstm(x)\n",
    "        if self.attention:\n",
    "            attention_w = F.softmax(self.attention_layer(x).squeeze(-1), dim=-1)\n",
    "            x = torch.sum(attention_w.unsqueeze(-1) * x, dim=1)\n",
    "        else:\n",
    "            x = x[:, -1]\n",
    "        return self.output_layers(x)\n",
    "\n",
    "\n",
    "##############################\n",
    "#     Conv2D Classifier\n",
    "#        (Baseline)\n",
    "##############################\n",
    "\n",
    "\n",
    "class ConvClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, latent_dim):\n",
    "        super(ConvClassifier, self).__init__()\n",
    "        resnet = resnet152(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(resnet.fc.in_features, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim, momentum=0.01),\n",
    "            nn.Linear(latent_dim, num_classes),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(batch_size * seq_length, -1)\n",
    "        x = self.final(x)\n",
    "        x = x.view(batch_size, seq_length, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "model = ConvLSTM(\n",
    "        num_classes=4,\n",
    "        latent_dim=256,\n",
    "        lstm_layers=1,\n",
    "        hidden_dim=1024,\n",
    "        bidirectional=True,\n",
    "        attention=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data,batch_size = 4,num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjaymoto75/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:522: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/10] [Batch 1339/1340] [Loss: 1.070266 (1.220971), Acc: 100.00% (49.48%)]--- Epoch 1 ---\n",
      "[Epoch 1/10] [Batch 1339/1340] [Loss: 1.024768 (1.184286), Acc: 100.00% (51.57%)]--- Epoch 2 ---\n",
      "[Epoch 2/10] [Batch 1339/1340] [Loss: 0.779323 (1.096459), Acc: 100.00% (63.64%)]--- Epoch 3 ---\n",
      "[Epoch 3/10] [Batch 1339/1340] [Loss: 0.757551 (1.022985), Acc: 100.00% (72.05%)]--- Epoch 4 ---\n",
      "[Epoch 4/10] [Batch 1339/1340] [Loss: 0.745787 (0.987584), Acc: 100.00% (75.76%)]--- Epoch 5 ---\n",
      "[Epoch 5/10] [Batch 1339/1340] [Loss: 0.745581 (0.966191), Acc: 100.00% (77.59%)]--- Epoch 6 ---\n",
      "[Epoch 6/10] [Batch 1339/1340] [Loss: 0.751061 (0.948501), Acc: 100.00% (79.51%)]--- Epoch 7 ---\n",
      "[Epoch 7/10] [Batch 791/1340] [Loss: 1.207551 (0.943110), Acc: 50.00% (79.96%)]]"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "import sys\n",
    "import cv2\n",
    "cls_criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_metrics = {\"loss\": [], \"acc\": []}\n",
    "    print(f\"--- Epoch {epoch} ---\")\n",
    "    for batch_i, (X, y) in enumerate(train_loader):\n",
    "        image_sequences = Variable(X.to(device), requires_grad=True)\n",
    "        labels = Variable(y.to(device), requires_grad=False)\n",
    "        optimizer.zero_grad()\n",
    "        #model.lstm.reset_hidden_state()\n",
    "        predictions = model(image_sequences)\n",
    "        loss = cls_criterion(predictions, labels)\n",
    "        acc = 100 * (predictions.detach().argmax(1) == labels).cpu().numpy().mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_metrics[\"loss\"].append(loss.item())\n",
    "        epoch_metrics[\"acc\"].append(acc)\n",
    "        batches_done = epoch * len(train_loader) + batch_i\n",
    "        batches_left = num_epochs * len(train_loader) - batches_done\n",
    "        sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f), Acc: %.2f%% (%.2f%%)]\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    batch_i,\n",
    "                    len(train_loader),\n",
    "                    loss.item(),\n",
    "                    np.mean(epoch_metrics[\"loss\"]),\n",
    "                    acc,\n",
    "                    np.mean(epoch_metrics[\"acc\"]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Empty cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
